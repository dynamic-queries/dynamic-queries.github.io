<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://dynamic-queries.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://dynamic-queries.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-07-31T08:20:04+00:00</updated><id>https://dynamic-queries.github.io/feed.xml</id><title type="html">Rahul Manavalan</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">(Original work) Random feature potentials</title><link href="https://dynamic-queries.github.io/blog/2023/random_models/" rel="alternate" type="text/html" title="(Original work) Random feature potentials" /><published>2023-07-22T08:00:00+00:00</published><updated>2023-07-22T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/random_models</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/random_models/"><![CDATA[<p>This is original work done as a part of my masters studies at TUM with <a href="http://christian.mendl.net/">Chrisitian Mendl</a> and <a href="https://fd-research.com/">Felix Dietrich</a>.</p>

<p><em>All ideas, figures, equations, examples and code, unless cited, are a work of my own.</em></p>

<h3 id="introduction">Introduction</h3>

<p>The importance of understanding the composition and behaviour of molecules is well established. Scientific disciplines such as chemistry, molecular biology, drug discovery and others, rely on molecular dynamics simulations as their core computational tool. 
Tradionally, MD has been and to some extent still is, dependent on empirical force fields for estimating the interaction forces among molecules.</p>

<p>These empirical models are a simplification at best. The “proper” way to evaluate the forces in a molecular dynamics simulation is to use first principle calculations that solves the stationary Schrodinger’s equations. But self-consistent loop sub-routines in most first principle calculations are too expensive to simulate molecules realistically for time domains where phenomena of interest emerge.</p>

<p>This brings us to the fore of research today, where machine learning algorithms (deep learning, especially) has been co-opted to “learn” the rules of quantum mechanics from huge clusters of molecular data, generated from first principle calculations. The models used are the usual suspects – neural networks, gaussian processes and other kernel methods.</p>

<p>In this work, we investigate the efficacy of random feature models as suitable surrogates for learning the ground state potentials of molecular systems.</p>

<p><em>I am aware that these two topics are esoteric in their own right. Therefore, I provide a quick interlude that discusses the notions of 1) Potential energy surfaces of molecular systems and 2) Random feature models as function approximation tools.</em></p>

<h3 id="interlude">Interlude</h3>

<h4 id="potential-energy-surfaces">Potential energy surfaces</h4>

<h4 id="random-feature-models">Random feature models</h4>
<p>Consider a sufficiently regular function \(f:x \mapsto y\). Then one can find bases functions \(\phi_i(x; \theta)\) with parameteric dependence on \(\theta\) such that</p>

\[\begin{align}
    f(x) \approx \sum_i a_i \phi_i(x; \theta)
\end{align}\]

<p>This is a common feature in most function approximation schemes. In particular, if one uses a neural network with one hidden layer:</p>

\[\begin{align}
    \label{eq:randomNN}
    f(x) \approx W_2 \: \rho \left(W_1 x  + b_1 \right)
\end{align}\]

<p>Tradionally, one randomly chooses the parameters which are subsequently trained with a gradient based algorithm. If one assumes that the weights \(\theta= \{W_1, W_2, b_1\}\) follow a probability distribution \(p_{\theta}\), during training this distribution is modified with every epoch.</p>

<p>Random feature models, on the otherhand,  sample \(\theta \sim \mathcal{U}_{\theta}\) from a uniform distribution (which requires no training). As surprising it is, it has been shown with uniform sampling of \(\theta\) in (\ref{eq:randomNN}), used in conjunction with \(\rho = \{cos, tanh, relu\}\) is a universal approximator. Further, one can define a litany of heuristics to define ad-hoc “probability” functions \(p_i\), which when used for sampling should yield more accurate approximations at the same cost.</p>

<p>Shown below is the function \(x \mapsto sin(x)\) trained with an ADAM optimizer and one sampled using the sampling algorithm (I will discuss this next.). The prediction from the sampling algorithm is one shot, requiring no iterative training training, that is characteristic of all gradient based training methods.</p>

<style>
    .column {
  float: left;
  width: 50.00%;
  margin : 0 0 0px 0px;
  padding: 2px;
}

/* Clear floats after image containers */
.row::after {
  content: "";
  clear: both;
  display: table;
}
</style>

<div class="row">
  <div class="column">
    <img style="width:100%" src="/assets/random_feature/single_output.svg" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:100%" src="/assets/random_feature/single_sampling.svg" alt="spline-sur" />
  </div>
</div>

<p>This is also the case for a multioutput function such as</p>

\[x \mapsto \begin{bmatrix} sin(4x) \\ cos(4x) \\ sin(4x) \: cos(4x)\end{bmatrix}\]

<div class="row">
  <div class="column">
    <img style="width:100%" src="/assets/random_feature/multi_output.svg" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:100%" src="/assets/random_feature/multi_sampling.svg" alt="spline-sur" />
  </div>
</div>

<h3 id="code">Code</h3>

<h5 id="random-feature-model-for-approximating-simple-functions">Random feature model for approximating simple functions</h5>

<script src="https://gist.github.com/dynamic-queries/7fe5162d9f355fe9e7414cf50a8fdfa0.js"></script>]]></content><author><name></name></author><category term="projects" /><category term="Surrogatization" /><summary type="html"><![CDATA[Inferring potential energy surfaces with random feature models.]]></summary></entry><entry><title type="html">(Original work) Bayesian Waveform Inversion</title><link href="https://dynamic-queries.github.io/blog/2023/bayesian_inversion/" rel="alternate" type="text/html" title="(Original work) Bayesian Waveform Inversion" /><published>2023-07-21T08:00:00+00:00</published><updated>2023-07-21T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/bayesian_inversion</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/bayesian_inversion/"><![CDATA[<p>This is original work done as a part of a student research project during my masters studies at TUM with <a href="https://fd-research.com/">Felix Dietrich</a>.</p>

<p><em>All ideas, figures, equations, examples and code, unless cited are a work of my own.</em></p>

<h3 id="introduction">Introduction</h3>

<p><strong>Waveform inversion</strong> is principally parameter estimation with three insights.</p>

<ol>
  <li>The parameter is a field over some domain \(\Omega\).</li>
  <li>Only partial observations of the quantity of interest are available.</li>
  <li>Observables are obtained from evaluating a family of wave propagation models.</li>
</ol>

<p>Consider a simple example of an acoustic wave equation.</p>

\[\begin{align}
    x \in \Omega \subset \mathbb{R} \\ 
    t \in \mathbb{R}^+ \\ 
    \frac{\partial}{\partial t}  u(x,t) = \frac{\partial^2}{\partial x^2} \left[c(x)^2 u(x,t)\right] \\ 
    u(x,0) = f(x) \\ 
    \frac{\partial }{\partial t}u(x,0) = g(x) \\
    u(y,t) = 0 \quad \forall y \in \delta \Omega
\end{align}\]

<p>Here \(f,g\) are suitably chosen initial conditions that would sustain elastic progation of a wave in \(\Omega\). In the context of waveform inversion, \(c(x)\) is the parameter and \(u(z,t) \: \forall z \in \hat{\Omega} \subset \Omega\) is the observable. For the sake of this article, consider the following example for \(c,u\).</p>

<style>
    .column {
  float: left;
  width: 50.00%;
  margin : 0 0 0px 0px;
  padding: 2px;
}

/* Clear floats after image containers */
.row::after {
  content: "";
  clear: both;
  display: table;
}
</style>

<div class="row">
  <div class="column">
    <img style="width:100%" src="/assets/bayesian/parameter.svg" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:100%" src="/assets/bayesian/observable.svg" alt="spline-sur" />
  </div>
</div>

<p>The goal is to reconstruct \(c(x)\) (<em>to the left</em>) from \(u(z,t)\) (<em>to the right</em>) measured at \(z=\{z_0, z_i\}\) where \(z_i \in \Omega\). The optimal \(c^{*}(x)\) is the minimizer to</p>

\[\begin{align}
  \label{eq:opt}
  \arg \min_{\hat{c} \in \mathcal{H}} \sum_{i}\| \hat{u}(z_i,t; \hat{c}(x)) - u(z_i,t)\|
\end{align}\]

<p>where \(\hat{u}(z_0,t)\) is evaluated by solving the acoustic wave equation using a robust finite difference method <em>(see code)</em>.</p>

<h3 id="parameter-estimation">Parameter estimation</h3>
<p>Estimating \(c(x)\) is non-trivial. Firstly, one has to choose an appropriate bases to parameterize this function. This of course varies on a case by case basis. For the example above, radial bases functions are good approximants. Therefore,</p>

\[\begin{align}
  c(x) = \sum_{i=1}^{N} a_i \phi_i (x) \\ 
  \phi_i(x) := \phi(x;x_i,\mu) = \exp\left[{-\left(\frac{x-x_0}{\mu}\right)^2}\right] \\ 
  N = 5
\end{align}\]

<p>Therefore the parameterization space is now \(\mathcal{A} \ni a\).</p>

<p>\(N\) is, by design, chosen sufficiently small to faciliate comparison with Bayesian optimization.</p>

<h4 id="frequentist-estimation">Frequentist estimation</h4>
<p>Problem (\ref{eq:opt}) is ill-posed in the sense of Hadamard. As a result, it is initially solved with tradional gradient based optimization algorithms with suitable regularization. The algorithms used in this pursuit, are briefy outlined and the “best possible” reconstructions are presented.</p>

<p>The reconstructions here, must be taken with a grain of salt, for it is entirely possible to come up with an excellent initial guess that could converge to the actual \(c(x)\) accurately.</p>

<p>Furthermore, it is observed that the reconstructions are sensitive to the choice of the type of regularization and the magnitude of the regularization constant. Common regularizations to an \(l_2\) loss includes weighted norms of inputs, finite difference of \(l_2\) loss with respect to time, fourier coefficients of \(l_2\) loss and so on.</p>

<p>In the presented cases, \(l_2\) Tikhonov regularization is used. That is</p>

\[\begin{align}
  \arg \min_{a \in \mathcal{A}} \sum_{i}\| \hat{u}(z_i,t; \hat{c}(x;a)) - u(z_i,t)\| + \lambda \| \hat{c}(x;a) \|
\end{align}\]

<p>The values of the regularization constants and the number of iterations used for each of these algorithms is indicated below.</p>

<h5 id="adam">ADAM</h5>

<p>This is a first order optimization algorithm introduced in 2014 by <a href="https://arxiv.org/abs/1412.6980">Kingma</a>. It is a variant of momentum based methods such as Adagrad and Nesterov optimization methods. Generically, if \(f_{\theta}\) is the objective function that needs to be optimized with respect to the parameters \(\theta\), it is done so as follows.</p>

<hr />
<p><strong><em>Algorithm</em></strong></p>

<p>Intitialize:</p>
<ol>
  <li>Constants \(\beta_1 = 0.9, \beta_2 = 0.999, \alpha = 0.001 , \epsilon=10^{-8}\)</li>
  <li>Initial positions and velocities \(m_0 = 0, v_0 = 0\)</li>
  <li>Make a “good initial guess” \(\theta_0\)</li>
</ol>

<p>For the \(t^{th}\) iteration:</p>

<ol>
  <li>Evaluate the gradient of the function \(g_t := \nabla_{\theta} f_{\theta_{t-1}}\)</li>
  <li>Compute “position” term : \(m_t = \frac{\beta_1}{1 - \beta_1 ^ t} m_{t-1} + \frac{1-\beta_1}{1 - \beta_1 ^ t} g_t\)</li>
  <li>Compute “momentum” term : \(v_t = \frac{\beta_2}{1 - \beta_2 ^ t} v_{t-1} + \frac{1-\beta_2}{1 - \beta_2 ^ t} g_t^2\)</li>
  <li>Update parameters : \(\theta_t = \theta_{t-1} - \alpha \frac{m_t}{\sqrt{v_t} + \epsilon}\)</li>
</ol>

<p>For \(t \in [1,\textrm{maxiters}] \subset \mathbb{N}\)</p>

<ol>
  <li>Repeat the four previous steps.</li>
</ol>

<hr />

<p>Using the previous algorithm and with a regularization constant \(\lambda = 2.01 \pm 10^{-4}\), one obtains the following reconstruction.</p>

<p><img style="width:60%" src="/assets/bayesian/recon_parameter_adam.svg" alt="spline-sur" /></p>

<h5 id="bfgs">BFGS</h5>

<p>One can also use, higher order optimization methods that make use of  \(\nabla_{\theta}(\nabla_{\theta} f_{\theta})\). Alas, computing the Hessian for every iteration is computationally expensive.
<a href="https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm">BFGS method</a> is a quasi-Netwon method which instead of computing the entire Hessian, approximates it with several evalutions of the gradient of \(f_{\theta}\).</p>

<p>Akin, to the previous section, one obtains a minimizer \(\theta^*\) for \(f_{\theta}\) as follows.</p>

<hr />
<p><strong><em>Algorithm</em></strong></p>

<p>Initialize:</p>

<ol>
  <li>Initial Hessian approximant and initial learning rate : \(B_0 = \mathcal{I}, \alpha_0 = 10^{-2}\)</li>
  <li>Make a “good initial guess” \(\theta_0\)</li>
</ol>

<p>For every \(k^{th}\) iteration:</p>

<ol>
  <li>Estimate new search direction, \(p_k\) by solving \(B_{k-1} p_k = -\nabla_{\theta} f_{\theta_{k-1}}\)</li>
  <li>Estimate step size, \(\alpha_k\) by solving \(\arg \min_{\alpha_k} f_{\theta}(\theta_{k-1} + \alpha_k p_k)\)</li>
  <li>Compute new direction \(\theta_{k} = \theta_{k-1} + \alpha_k p_k\)</li>
  <li>Estimate update vector \(y_k = \nabla_{\theta} f(\theta_{k}) - \nabla_{\theta} f(\theta_{k-1})\)</li>
  <li>Let \(s_k = \alpha_k p_k\)</li>
  <li>Update Hessian approximant \(B_{k} = B_{k-1} + \frac{y_k y_k^T}{y_k^T s_k} + \frac{B_{k-1} s_k^T s_k B_{k-1}^T}{s_k^T B_{k-1} s_k}\)</li>
</ol>

<p>For \(k \in [1,\textrm{maxiters}] \subset \mathbb{N}\)</p>

<ol>
  <li>Repeat the six previous steps.</li>
</ol>

<hr />

<p>Using the previous algorithm and with a regularization constant \(\lambda = 0.399 \pm 10^{-3}\), one obtains the following reconstruction.</p>

<p><img style="width:60%" src="/assets/bayesian/recon_parameter_bfgs.svg" alt="spline-sur" /></p>

<h5 id="issues">Issues</h5>

<p>There are two difficulties that one encounters with the frequentist estimation while treating an ill-conditioned problem such as this one.</p>

<ol>
  <li>Good initial guesses are vital for fast convergence.</li>
  <li>Estimation is very sensitive to the value of \(\lambda\).</li>
</ol>

<p>Elaborating on (2), consider the following reconstructions with BFGS optimization for \(\lambda = 0.397,\:0.399\:,0.40\:,0.402\)  respectively.</p>

<div class="row">
  <div class="column">
    <img style="width:80%" src="/assets/bayesian/bfgs_0.397.svg" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:80%" src="/assets/bayesian/bfgs_0.399.svg" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:80%" src="/assets/bayesian/bfgs_0.4.svg" alt="spline-sur" />
  </div>
  <div class="column">
    <img style="width:80%" src="/assets/bayesian/bfgs_0.402.svg" alt="spline-sur" />
  </div>
</div>

<p>Such sensitivity makes any practical grid search (which is performed at a much higher resolution) useless, questioning the practicality of such optimization routines for practical applications.</p>

<h4 id="bayesian-estimation">Bayesian estimation</h4>
<p>Plagued by the ill-posedness of frequentist estimation, it seems appropriate to turn to Bayesian estimation. Granted, one does need a good prior for Bayesian methods to make computational sense, however Bayesian inverse problems are well posed (As highlighted in <a href="https://www.cambridge.org/core/journals/acta-numerica/article/abs/inverse-problems-a-bayesian-perspective/587A3A0D480A1A7C2B1B284BCEDF7E23">Stuart</a>).</p>

<p>Appropriately, the Bayes rule for this inference problem can be written as</p>

\[\begin{align}
  p(a | u(z)) = \frac{p(u(z)|a)\:p(a)}{p(u(z))}
\end{align}\]

<p>It is known that, esimating the marginal density \(p(u(z))\) is computationlly intractable. Therefore, in what follows two classes of Bayesian state estimation methods – MCMC and HMC (and its variants) are used to estimate \(p(a|u(z))\) from which one can compute the expectation, variance and other higher order moments of \(\hat{c}(x;a)\).</p>

<h5 id="markov-chain-monte-carlo">Markov Chain Monte carlo</h5>

<h6 id="metropolis-hasting-sampling">Metropolis Hasting sampling</h6>

<h6 id="gibbs-sampling">Gibbs sampling</h6>

<h5 id="hamiltonian-monte-carlo">Hamiltonian Monte carlo</h5>

<h6 id="plain-hmc">Plain HMC</h6>

<h6 id="no-u-turn-sampling-hmc">No U-turn sampling HMC</h6>

<h3 id="code">Code</h3>

<p>These results can be reproduced with the following code. <em>(Documentation coming soon.)</em></p>

<script src="https://gist.github.com/dynamic-queries/3473ab7b87744cd1ad07cddd74eabd03.js"></script>]]></content><author><name></name></author><category term="projects" /><category term="InverseProblems" /><summary type="html"><![CDATA[Full waveform inversion with MCMC, HMC, NUTS]]></summary></entry><entry><title type="html">(Original work) Latent space Bayesian Waveform Inversion</title><link href="https://dynamic-queries.github.io/blog/2023/latent_space_bayesian_FWI/" rel="alternate" type="text/html" title="(Original work) Latent space Bayesian Waveform Inversion" /><published>2023-07-21T08:00:00+00:00</published><updated>2023-07-21T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/latent_space_bayesian_FWI</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/latent_space_bayesian_FWI/"><![CDATA[<h3 id="introduction">Introduction</h3>
<p>Waveform inversion concerns reconstructing velocity fields from remotely measured wave signals. It has been common practice to optimize for velocity fields parameterized as indicator functions. Considering the ill-posed nature of inversion, the trend is to turn to Bayesian inversion methods, which can be formulated as well posed problems. In addition, Bayesian methods provides one, an estimate of the uncertainity in prediction, which is ever so crucial, should waveform inversion be used for resource sensitive applications.</p>

<p>Despite all evidences pointing towards adopting Bayesian parameter estimation for waveform inversion, it is still not common practice in the FWI community; largely because Bayesian methods are extremely expensive for models with large parameters.</p>

<p>In this work, we use the time honored practice of seeking latent space representations for families of velocity fields. Subsequently we demonstrate that Bayesian waveform inversion in these latent spaces becomes computational tractable.</p>

<h3 id="latent-spaces">Latent spaces</h3>

<p>We begin with some observations. More specifically, we consider some velocity fields that are commonplace in the inversion literature – seismic and otherwise.</p>

<h4 id="fourier-space-representations">Fourier space representations</h4>
<p>Initially, we seek Fourier space latent representation of the fields. Initial observations indicate that one can do away with more than 80% of the Fourier bases functions and still obtain a qualitatively good estimate of the velocity field at the cost of mapping to and back from the Fourier space. (This is realized with <strong>fft</strong>.)</p>

<h5 id="primitives">Primitives</h5>

<style>
    .column {
  float: left;
  width: 50.00%;
  margin : 0 0 0px 0px;
  padding: 20px;
}

/* Clear floats after image containers */
.row::after {
  content: "";
  clear: both;
  display: table;
}
</style>

<div class="row">
  <div class="column">
    <img style="width:100%" src="/assets/latent/circle.svg" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:100%" src="/assets/latent/square.svg" alt="spline-sur" />
  </div>
</div>

<div class="row">
   <div class="column">
    <img style="width:100%" src="/assets/latent/2circle.svg" alt="spline-sur" />
    </div>
   <div class="column">
    <img style="width:100%" src="/assets/latent/circle4.svg" alt="spline-sur" />
  </div>
</div>

<h5 id="velocity-classes">Velocity classes</h5>

<div class="row">
  <div class="column">
    <img style="width:100%" src="/assets/latent/simple.svg" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:100%" src="/assets/latent/complex.svg" alt="spline-sur" />
  </div>
</div>

<div class="row">
   <div class="column">
    <img style="width:100%" src="/assets/latent/Csimple.svg" alt="spline-sur" />
    </div>
   <div class="column">
    <img style="width:100%" src="/assets/latent/Ccomplex.svg" alt="spline-sur" />
  </div>
</div>

<p>One should note that, for discontinuous functions, high frequency Fourier bases functions become increasingly relevant. While, this was not so debilitating for the examples in the Primitive section, it is clear that seismic velocity fields cannot be robustly parameterized.</p>

<h4 id="learned-latent-space-representations">Learned latent space representations</h4>

<p>This leads us to learning the latent space representations using <a href="https://arxiv.org/abs/1906.02691">variational autoencoders</a>.</p>]]></content><author><name></name></author><category term="projects" /><category term="InverseProblems" /><summary type="html"><![CDATA[Waveform inversion on parsimonious models]]></summary></entry><entry><title type="html">SINDY - Sparse Identification of non-linear dynamical systems.</title><link href="https://dynamic-queries.github.io/blog/2023/sindy/" rel="alternate" type="text/html" title="SINDY - Sparse Identification of non-linear dynamical systems." /><published>2023-07-20T08:00:00+00:00</published><updated>2023-07-20T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/sindy</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/sindy/"><![CDATA[<p>This work is a re-creation of a symbolic regression method from <a href="https://www.pnas.org/doi/10.1073/pnas.1517384113">Brunton and Kutz</a>. The article is meant to be a quick summary of this method.</p>

<p><em>All figures, equations, examples and code are a work of my own. Only the idea has been borrowed from the paper.</em></p>

<h3 id="introduction">Introduction</h3>

<p>Data driven methods are the modern day equivalent to Poincare’s work in the 19th century. A characteristic feature of these methods is determining the equations of motion underlying a given dynamical system, exclusively from time dependent observables, measured from that system.</p>

<hr />
<p>Consider this generic form of a dynamical system.</p>

\[\begin{align}
    \frac{dx(t)}{dt} = f(x(t)) \\ 
    x(0) = x_0 \in \mathbb{R}^d
\end{align}\]

<p>Given \(X := [x(t_1),x(t_2),...,x(t_N)]\), we would like to infer \(f\) either symbolically or numerically.</p>

<hr />

<h3 id="sindy">SINDY</h3>

<p>SINDY is a symbolic regression algorithm that identifies \(f\), provided one has access to the most apposite bases set, say \(\mathcal{B}\), that spans the function space \(\mathcal{F} \ni f\). It’s usually the case that for most systems, knowledge of \(\mathcal{B}\) is sparse at best; raising serious questions about the practicality of this method for data arising from real world applications. Nonetheless it is a nice method where one has expert knowledge about the system. The algorithm has four main steps.</p>

<hr />

<p><strong><em>Algorithm</em></strong></p>

<ol>
  <li>
    <p>Evaluate \(\dot{X} := [\frac{dx(t_1)}{dt},\frac{dx(t_2)}{dt},...,\frac{dx(t_N)}{dt}] \in \mathbb{R}^{d \times N}\) using a sufficiently robust finite difference scheme.</p>
  </li>
  <li>
    <p>Choose a bases set \(\mathcal{B} := \{b_1(x), b_2(x), ..., b_k(x)\}\) of functions.</p>
  </li>
  <li>
    <p>Evalute \(\Theta(X) := \begin{bmatrix} b_1(x(t_1)) &amp;&amp; ... &amp;&amp; b_1(x(t_N)) \\ \vdots &amp;&amp;  &amp;&amp; \vdots \\  b_k(x(t_1)) &amp;&amp; ... &amp;&amp; b_k(x(t_1)) \\ \end{bmatrix} \in \mathbb{R}^{k \times d \times N}\)</p>
  </li>
  <li>
    <p>If \(\dot{X} = K \: \Theta(X)\) where \(K \in \mathbb{R}^{k}\). Solve for \(K\) using linear least squares and a sparsity inducing regularizer.</p>
  </li>
</ol>

<hr />

<h3 id="examples">Examples</h3>

<p>Here the algorithm is applied to commonly recurring examples in literature – Lorenz 69 attractor and a Lotka-Volterra model.</p>

<style>
    .column {
  float: left;
  width: 50.00%;
  margin : 0 0 0px 0px;
  padding: 2px;
}

/* Clear floats after image containers */
.row::after {
  content: "";
  clear: both;
  display: table;
}
</style>

<h4 id="lorenz-69-attractor">Lorenz 69 attractor</h4>

<p>The following non-linear ODE with a specific configuration of parameters has been the poster-child of chaotic attractors for several decades now.</p>

\[\begin{align}
  \dot{x}(t) = \sigma (y - x)\\
  \dot{y}(t) = x(\rho - z) - y\\
  \dot{z}(t) = xy - \beta z\\ 
  \sigma = 10 \\ 
  \rho = 28 \\ 
  \beta = \frac{8}{3} \\ 
  \begin{bmatrix} x_0 \\ y_0 \\ z_0  \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} \\ 
  t \in [0,100]
\end{align}\]

<p>Viewed clockwise, the figure below shows i) Trajectories of the attractor obtained from a numerical integrator, ii) Time derivative of the trajectories, iii) Chosen bases functions \(\mathcal{B}\) and the corresponding weights and iv) the reconstructed attractor.</p>

<div class="row">
  <div class="column">
    <img style="width:100%" src="/assets/sindy/Traj_Lorenz.png" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:100%" src="/assets/sindy/Vel_Lorenz.png" alt="spline-sur" />
  </div>
</div>

<div class="row">
  <div class="column">
    <img style="width:80%" src="/assets/sindy/Lorenz_Coeffs.png" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:100%" src="/assets/sindy/Remade_Lorenz.png" alt="spline-sur" />
  </div>
</div>

<hr />

<h4 id="lotka-volterra-model">Lotka Volterra model</h4>

<p>Prey predator models are the staple for several ecological, biological and even plasma physical applications. A common model with a single species of prey and predator is shown below.</p>

\[\begin{align}
  \dot{x}(t) = \alpha x(t) + \beta x(t)y(t) \\ 
  \dot{y}(t) = \gamma x(t)y(t) - \delta y(t) \\
  \alpha = 0.7 \\
  \beta = -0.3 \\ 
  \gamma = -0.3 \\ 
  \delta = 0.4 \\ 
  \begin{bmatrix} x_0 \\ y_0  \end{bmatrix} = \begin{bmatrix} 1 \\ 1  \end{bmatrix} \\ 
  t \in [0,100]
\end{align}\]

<p>Akin to the Lorenz attractor, the SINDY algorithm is used to infer the bases weights and the trajetories are then reconstructed.</p>

<div class="row">
  <div class="column">
    <img style="width:100%" src="/assets/sindy/Traj_Lotka.png" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:100%" src="/assets/sindy/Vel_Lotka.png" alt="spline-sur" />
  </div>
</div>

<div class="row">
  <div class="column">
    <img style="width:80%" src="/assets/sindy/LV_Coeffs.png" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:100%" src="/assets/sindy/Remade_Lotka.png" alt="spline-sur" />
  </div>
</div>

<hr />

<h3 id="code">Code</h3>

<p>These results can be reproduced with the following code. <em>(Documentation coming soon.)</em></p>

<script src="https://gist.github.com/dynamic-queries/3edf052b43ec90467dc48f91fe3b552b.js"></script>]]></content><author><name></name></author><category term="projects" /><category term="SystemsIdentification" /><summary type="html"><![CDATA[Symbolic regression.]]></summary></entry><entry><title type="html">(Original work) Full Waveform inversion</title><link href="https://dynamic-queries.github.io/blog/2023/fwi/" rel="alternate" type="text/html" title="(Original work) Full Waveform inversion" /><published>2023-07-20T08:00:00+00:00</published><updated>2023-07-20T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/fwi</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/fwi/"><![CDATA[<p>This is original work done as a part of a student research project during my masters studies at TUM with <a href="https://fd-research.com/">Felix Dietrich</a>. Part of this work was presented at <a href="https://meetings.siam.org/sess/dsp_talk.cfm?p=125969">SIAM CSE 2023 in Amsterdam</a></p>

<p><em>All ideas, figures, equations, examples and code are a work of my own.</em></p>

<h3 id="introduction">Introduction</h3>

<p>Ultrasonic imaging is a versatile tool for visualizing the subsurface properties of structures, which are not readily available for physical observation. In the last few decades this method has proliferated all walks of life, with applications ranging from medical imaging to non-destructive testing in engineering and the like. There have also been several adapatations such as photo-acoustic imaging which is a crucial tool in imaging microscopic elements of biological matter.</p>

<p>Here I am interested in non-destructive testing for engineering structures. Consider the idealized scenario illustrated below.</p>

<style>
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}
</style>

<p><img style="width:70%" class="center" src="/assets/FWI_edit/outline.svg" alt="spline-sur" /></p>

<p>There is a rectangular domain with a defect in it. One does not know apriori how the defect looks like. But one has access to sensors measuring the amplitude of waves on the surface of the domain. Full waveform inversion concerns reconstructing the shape of the defect from these measured signals.</p>

<h3 id="method">Method</h3>

<p>In an ideal setting, the waves inside the domain \(\Omega\) can be assumed to propagate with velocity \(\forall x \in \Omega \quad c: x \mapsto \mathbb{R}\) and evolving in accordance with the acoustic wave equation.</p>

\[\begin{align}
    \frac{\partial^2 u(x,t)}{\partial t^2} = \frac{\partial^2 }{\partial x^2}\left[c(x)^2 u(x,t)\right] \\ 
    u(x,0) = f(x) \\ 
    \frac{\partial u(x,0)}{\partial t} = g(x) \\ 
    u(y,t) = 0 \:\: \forall y \in \delta \Omega
\end{align}\]

<p>Here, I assume that the shape, \(s(x)\) of the defect is related to the velocity \(c(x)\) through \(s(x) = \phi(c(x))\) for some known invertible function \(\phi\). From this, it can be argued that inferring \(c(x)\) from reference measurement \(u_m(z,t) \:\: \forall z \in \Omega_s\) is, in effect, full-wave form inversion. Or equivalently one seeks the minimizer for the following:</p>

\[\begin{equation}
    \arg \min_{c \in \mathcal{H}} \|u(z,t;c(x)) - u_m(z,t)\|
\end{equation}\]

<h3 id="procedure">Procedure</h3>

<p>I illustrate my approach to this problem with a synthetic example.</p>]]></content><author><name></name></author><category term="projects" /><category term="InverseProblems," /><category term="Surrogatization" /><summary type="html"><![CDATA[Computational Non-destructive testing]]></summary></entry><entry><title type="html">Learning Hamiltonians with GP</title><link href="https://dynamic-queries.github.io/blog/2023/hamiltonian/" rel="alternate" type="text/html" title="Learning Hamiltonians with GP" /><published>2023-07-20T08:00:00+00:00</published><updated>2023-07-20T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/hamiltonian</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/hamiltonian/"><![CDATA[<p>This work is a re-creation of a method for learning the Hamiltonian of a dynamical system from <a href="https://pubs.aip.org/aip/cha/article/29/12/121107/1027304/On-learning-Hamiltonian-systems-from-data">Bertlan and Dietrich</a>.</p>

<p><em>All figures, equations, examples and code are a work of my own. Only the idea has been borrowed from the paper.</em></p>

<h3 id="introduction">Introduction</h3>

<p>Most physical systems lend themselves to description using Hamiltonian mechanics. Briefly put:</p>

<p>For an \(N\) body system living in \(d\) dimensions, the classical mechanical state of the system is described sufficiently using a pair of canonical coordinates \(q,p \in \mathbb{R}^{dN}\). For an evolutionary system, the canonical coordinates depend on time. Evidently, such evolutions are governed by Newton’s equations. However, Netwon’s equation are bulky to compute, analyze for large \(N\). This problem was recognized by Hamilton and subsequently led to what is now referred to the Hamiltonian formulation of classical mechanics.</p>

<p>Hamilton postulated that there exists a function \(H : q \times p \mapsto \mathbb{R}\) – the Hamiltonian. The evolution equations in this framework were given by the following PDE:</p>

\[\begin{align}
    \label{eq:Ham}
    \frac{\partial}{\partial t} q(t) = \frac{\partial H}{\partial p}\\ 
    \frac{\partial}{\partial t} p(t) = -\frac{\partial H}{\partial q}
\end{align}\]

<p>Or more compactly</p>

\[{\partial_t} z := {\partial_t}\begin{bmatrix} q \\ p \end{bmatrix} = \begin{bmatrix} \mathcal{0} &amp;&amp;  \mathcal{I} \\ -\mathcal{I} &amp;&amp; \mathcal{0} \end{bmatrix} \partial_{\begin{bmatrix} q &amp; p \end{bmatrix}^{T}} H(q,p)\]

<p>Given observations \(\left([q(t_1),q(t_2),...,q(t_m)],[p(t_1),p(t_2),...,p(t_m)]\right)\), I am interested in learning \(H\) using a Gaussian process.</p>

<h3 id="method">Method</h3>

<p>Let \(H(z) \sim \mathcal{GP(\mu_z,\kappa(z,z'))}\).</p>

\[\begin{align}
  \mathbb{E}(H(z^*)) = k(z^* ,z)\: k(z ,z')^{-1}\:H(z') \\
  \frac{\partial}{\partial z^*}\mathbb{E}(H(z^*)) = \mathcal{J}^{-1} \frac{\partial z^*}{\partial t} \\
  \left[\frac{\partial}{\partial z^*}k(z^* ,z)\right]\: \left[k(z ,z')\right]^{-1}\:H(z') =\mathcal{J}^{-1} \frac{\partial z^*}{\partial t} 
\end{align}\]

<p>Here, \(z^*\) and \(\frac{\partial z^*}{\partial t}\) are known reference points. \(z'\) are the validation points where the derivative of the Hamiltonian is not known. In order to determine this, one proceeds according to the following algorithm.</p>

<hr />

<p><strong><em>Algorithm</em></strong></p>

<p>Given</p>
<ol>
  <li>Snapshots \(z^*(t_d) = \left[z^*(1), z^*(2) ... z^*(T) \right]\)</li>
  <li>Query points \(z_1, z_2, ...,z_N\)</li>
  <li>Covariance kernel \(\kappa\)</li>
</ol>

<p>Do</p>
<ol>
  <li>Evaluate \(\frac{d z^*}{dt}\) using any higher order finite difference scheme.</li>
  <li>Setup \(b:=\mathcal{J}^{-1} \: \frac{d z^*}{dt}\)</li>
  <li>Compute \(K_{zz'} \in \mathbb{R}^{N \times N}\) where \(K_{zz'}^{i,j} := \kappa(z^i,z^j)\)</li>
  <li>Invert \(K_{zz'}\)</li>
  <li>Compute \(M \in \mathbb{R}^{2dN \times N}\) where \(M^{:,j} := \frac{\partial}{\partial z^*} \kappa(z^*,z)\)</li>
  <li>Evaluate inference matrix \(I := M K_{zz'}^{-1}\)</li>
  <li>Solve \(I H_{z'} = b\) for \(H_{z'}\)</li>
</ol>

<hr />

<h3 id="examples">Examples</h3>

<h5 id="simple-non-linear-pendulum">Simple non-linear pendulum</h5>

<p>A pendulum is a prototypical example of a Hamiltonian dynamical system. As shown below it is a one-dimensional dynamical system, described by the amplitude of oscillations \(\theta\) and its time derivative \(\dot{\theta}\).</p>

<style>
    .column {
  float: left;
  width: 50.00%;
  margin : 0 0 0px 0px;
  padding: 2px;
}

/* Clear floats after image containers */
.row::after {
  content: "";
  clear: both;
  display: table;
}
</style>

<style>
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}
</style>

<p><img style="width:15%" class="center" src="/assets/hamiltonian/pendulum.svg" alt="spline-sur" /></p>

<p>The equations of motion of a pendulum is the classical equation 
\(\begin{align}
    \frac{d^2 \theta}{dt^2} = -\frac{g}{l} sin(\theta)   
\end{align}\)
which when solved for an ensemble of initial conditions, results in its phase space. The phase space and its tangent bundle (its derivative) is shown below.</p>
<div class="row">
  <div class="column">
    <img style="width:100%" src="/assets/hamiltonian/mini_sampled_trajectories.svg" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:100%" src="/assets/hamiltonian/mini_sampled_velocities.svg" alt="spline-sur" />
  </div>
</div>

<p>Note that all the points in the images above satisfies the PDE in (\ref{eq:Ham}). Therefore, one could uniformly draw samples from this space to learn its Hamiltonian. (<em>The figures above show 500 unique samples in red.</em>)</p>]]></content><author><name></name></author><category term="projects" /><category term="SystemsIdentification" /><summary type="html"><![CDATA[Inferring invariants of dynamical systems with gaussian processes.]]></summary></entry><entry><title type="html">Multi-output Gaussian processes</title><link href="https://dynamic-queries.github.io/blog/2023/mogp/" rel="alternate" type="text/html" title="Multi-output Gaussian processes" /><published>2023-07-20T08:00:00+00:00</published><updated>2023-07-20T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/mogp</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/mogp/"><![CDATA[<h3 id="introduction">Introduction</h3>
<p>Gaussian processes are a popular <strong>Bayesian</strong> <strong>learning</strong> framework.</p>

<p>A typical <em>learning problem</em> consists of approximating an approximation \(\hat{f}\) to the function \(f:x \mapsto y \quad \forall x,y \in \Omega \subset \mathbb{R}^d\), given evaluations of the function at certain input points, that is \(\mathcal{D} = \{(x_i, f_i) : i \in \mathbb{N}\}\).</p>

<p><em>Bayesian inference</em> assumes that the input \(x\) and the outputs \(y\) are random variables with some densities \(P_x, P_y\) respectively. These densities are related by</p>

\[P(x,y) = P(y | x) P(x) = P(x | y) P(y)\]

<p>Note that we do not have a closed form for any of these probability functions and estimating probability densities from data is in itself a hard problem. Instead, we make guesses about what these functions might be. Historically, each of these probability functions have their own names, namely</p>

<ol>
  <li>\(P(y)\) – Prior</li>
  <li>\(P(x | y)\) – Likelihood</li>
  <li>\(P(x)\) – Marginal</li>
  <li>\(P(y | x)\) – Posterior</li>
</ol>

<p>Commonly, one assumes a functional form for the prior. The posterior is the updated version of the prior, once we have seen the reference data.</p>

<h3 id="single-output-gp">Single Output GP</h3>

<p>Further, samples \(y,x\) are assumed to be related via</p>

\[y = f(x)\]

\[f:x \mapsto y \quad \forall x,y \in \Omega\]

<p>Here \(f\) is assumed to be any non-linear function. One can project \(x\) onto a higher dimensional space spanned by the bases generated from a kernel function \(\kappa\), also known as the <strong>kernel trick</strong>, where the non-linearity of \(f\) is transformed into a linearity. Without any loss of generality, we can assume that any non-linear function can be transformed into a linear map.</p>

\[y = Wx + b\]

\[W \in \mathbb{R}^{d \times d} \: \textrm{ and } b \in \mathbb{R}^d\]

<p>With this parameteric form for the output, defining a prior for the weights, \(z := \begin{bmatrix} W &amp; b \end{bmatrix}\) is equivalent to defining a prior for the output, \(y\). Therefore in a single output GP, there are three random variables \(x,y,z\). Their joint probability distribution is given as</p>

\[\begin{align}
    P(x,y,z) = P(x,y|z) P(z) = P(z|x,y) P(x,y) \\
 P(z|x,y) = \frac{P(x,y|z) P(z)}{P(x,y)} \\ 
 = \frac{P(y|x,z) P(x,z)}{P(x,y)} \\
  = \frac{P(y|x,z) P(x)P(z)}{P(y|x)P(x)}\\
 P(z|x,y) \propto P(y|x,z) P(z)
 \end{align}\]

<p>Here, \(z \sim \mathcal{N}(0, \Sigma_z)\), that is</p>

\[\begin{align}
    P(z) = \frac{1}{\sqrt{(2\pi)^d |\Sigma_z|}} \exp\left(-\frac{1}{2} z^T \Sigma_z^{-1} z\right)
\end{align}\]

<p>Further, the likelihood is defined with the assumption that each component of \(x\) is independent of one another. This is why this inference procedure is called single output Gaussian process. That is</p>

\[\begin{align}
    P(y|x,z) = \prod_{i=1}^{d} \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{1}{2}\left(\frac{y_i - (W_i x_i + b_i)}{\sigma}\right)^2\right) \\ 
    P(y|x,z) = \frac{1}{(\sqrt{2\pi \sigma^2})^d} \exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^{d} (y_i - (W_i x_i + b_i))^2\right) \\ 
    = \frac{1}{\sqrt{(2\pi)^d |\Sigma_y|}} \exp\left(-\frac{1}{2}(y - (W x + b))^T \Sigma_y^{-1} (y - (W x + b))\right)
\end{align}\]

<p>From this \(P(y|x,z)\) is a multivariate normal distribution \(\mathcal{N}(Wx+b, \Sigma_y)\).</p>

<p>Further one can write down the posterior as</p>

\[\begin{align}
    P(z|x,y) \propto P(y|x,z) P(z) \\ 
    = \frac{1}{(2\pi)^d \sqrt{|\Sigma_z||\Sigma_y|}} \exp\left(-\frac{1}{2} \left[z^T \Sigma_z^{-1} z + (y - (W x + b))^T \Sigma_y^{-1} (y - (W x + b)) \right]\right) \\ 
    = \frac{1}{(2\pi)^d \sqrt{|\Sigma_z||\Sigma_y|}} \exp\left(-\frac{1}{2} \left[z^T \Sigma_z^{-1} z + (y - z x')^T \Sigma_y^{-1} (y - z x') \right]\right)
\end{align}\]]]></content><author><name></name></author><category term="projects" /><category term="SystemsIdentification" /><summary type="html"><![CDATA[Learning mutivariate dynamical systems with GPs]]></summary></entry></feed>