<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://dynamic-queries.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://dynamic-queries.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-04-29T17:44:19+00:00</updated><id>https://dynamic-queries.github.io/feed.xml</id><title type="html">Rahul Manavalan</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Full waveform inversion with Neural Operators.</title><link href="https://dynamic-queries.github.io/blog/2023/fwi/" rel="alternate" type="text/html" title="Full waveform inversion with Neural Operators." /><published>2023-04-24T15:12:00+00:00</published><updated>2023-04-24T15:12:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/fwi</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/fwi/"><![CDATA[<hr />

<h2 id="introduction">Introduction</h2>

<p>Full waveform inversion (FWI) is an effective technique to determine the subsurface mechanical properties of geological masses and engineering structures. It differs from other tomographic techniques in making use of the entire wavefield – the amplitude, phase and frequency, to estimate features with high resolution. It consists of minimizing the distance between a measured signal and an artifically simulated one.</p>

<p>Formally, let \(x \in \Omega\) be the domain of interest and let \(m_{o}(x)\) be an observable defined over \(\Omega\). Let \(A : m(x) \times x \mapsto u(x)\) be an operator, mapping the observable and the domain to the wavefield \(u\). (\(A\) is also known as the forward map.) If \(u_{o}(x)\) is the wavefield corresponding to \(m_{o}(x)\), then FWI is principally a minimization problem defined as:</p>

<p>\(\begin{equation}
    \label{eq:fwi}
    \arg \min_{m \in \mathcal{M}} ||u_{o}(x) - A(m(x),x)||_p
\end{equation}\)
where \(\mathcal{M}\) is the hypothesis space of candidate observables explored with respect to a suitable \(p\) norm.</p>

<p>Since its first formulation in 1977, much has been discovered about FWI with respect to the efficacy of optimizers (SGD vs quasi-Newton vs Newton vs Truncated Newton) and the choice of measures (\(L_p\) norm vs Wasserstein measure). Furthermore, advances in computing has also benefitted this discipline; with highly parallel solvers that operate on very large time and length scales.</p>

<p>Recently, machine learning based surrogates for forward maps are gaining traction due to their computational efficacy. Surrogates based on Neural Operators in particular have recently shown much promise for several applications in literature. In this work, we demostrate waveform inversion with such surrogates and outline their applicability for highly expensive optimization frameworks based on Bayesian inversion or optimal control.</p>

<hr />
<h2 id="problem-setup">Problem Setup</h2>
<p>Let \(\Omega \subset \mathbb{R}^2\).</p>

<p>For a reference observable (velocity) \(m_0:\Omega \mapsto \mathbb{R}\), let the waveform be \(u_0(x,t) = A(m_0(x),x) \:\: \forall x \in \Omega\).</p>

<p>Assuming that from an initial measurement (oracle), \(u_0(x_s,t) \:\: \forall x_s \subset x\), is given, we are interested in numerically estimating \(m_0(x)\).</p>

<hr />
<p>For instance, one could imagine the reference velocity \(m_0(x)\) as being :</p>
<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/FWI/reference_defect.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>and the corresponding waveform sampled at seven different locations (sensors) in \(\Omega\) as</p>
<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/FWI/reference_waveform.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<hr />

<p>The optimization problem in \((\ref{eq:fwi})\) minimizes the distance betweem the above waveform (say) \(u_{m_0}\) and a waveform \(u\) obtained from a guessed observable \(m(x)\). This computation is iterated until \(\|u_{m_0}(x_s,t)-u(x_s,t)\|_2 \mapsto \epsilon\), where \(\epsilon\) is close to machine precision. Applications such as this, which repeatedly invoke an expensive computational routine such as the waveform map \(A\), are referred to as outerloop applications.</p>

<p>Depending upon the choice of basis used in discretization, computational routines involving \(A\) can be very expensive. Alternatively, we use Neural Operator surrogates for approximating \(A\). In the figure, The predictions of the neural operator (say) \(A_{NO}\) for a sample input observable is juxtaposed with the result from the full order simulation.</p>]]></content><author><name></name></author><category term="research" /><category term="FWI" /><summary type="html"><![CDATA[A novel approach to an old problem.]]></summary></entry><entry><title type="html">Inferring Linear Operators.</title><link href="https://dynamic-queries.github.io/blog/2023/lin_op/" rel="alternate" type="text/html" title="Inferring Linear Operators." /><published>2023-04-24T15:12:00+00:00</published><updated>2023-04-24T15:12:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/lin_op</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/lin_op/"><![CDATA[]]></content><author><name></name></author><category term="research," /><category term="operator-learning" /><category term="OpReg" /><summary type="html"><![CDATA[Computationally Tractable Operator Learning.]]></summary></entry><entry><title type="html">Trajectory Based Machine Learning Potentials.</title><link href="https://dynamic-queries.github.io/blog/2023/mlp/" rel="alternate" type="text/html" title="Trajectory Based Machine Learning Potentials." /><published>2023-04-24T15:12:00+00:00</published><updated>2023-04-24T15:12:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/mlp</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/mlp/"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Any physical system is characterized by a certain set of properties. It is these properties that define what that physical object is. Metals, for instance, are electrically and thermally conductive. Ceramics have extreme resitance to abrasion, electrical and thermal stresses. Biological tissues exhibit among other things properties such as regeneration. There are seemingly endless objects and endless properties that they posses.</p>

<p>It is all the more impressive that a finite set of elements and compositions thereof, constitue this rich tapestry of physical systems. Naturally, this question intrigued the ancients and continues to intrigue the modern day researcher. What we do know is, interactions among these fundamental building blocks are crucial to explaining these emergent phenomena.</p>

<p>In that pursuit there is a whole branch of mathematics – potential theory, devoted to finding a unifying framework for analyzing these potentials. There are countless physicists (theoretical and experimental), finding the <strong>“right interaction potential”</strong> for a specific system. And finally, there are computational scientists working towards effective computational procedures for calculating these potentials.</p>

<p>Until recently, a typical workflow for estimating the interaction potential for a system \(\mathcal{S}\) would look like :</p>
<ol>
  <li>Experimental data acquisition and validation by an experimentalist.</li>
  <li>Data analysis and theorization by physicists.</li>
  <li>In-silico validation of the theory by a computational scientist.</li>
</ol>

<p>In the age of machine learning, steps 2,3 are aggregated and serves as an <strong>“automation”</strong> of scientific discovery. While the results from ML-Potentials are certainly impressive, there are also unexplainable. “Unexaplainability” of machine learning models often has a negative connotation, and in some sense it is justified. But it also indicates that there exist more accurate models that could potentially be discovered using current mathematical frameworks. This work is a step in that direction while leveraging classical statistical models.</p>

<h2 id="problem-setup">Problem setup</h2>
<p>Consider a system \(\mathcal{S}\) with \(n\) identical interacting particles that live in a \(d\) dimensional phase space with positions \(q \in \mathbb{R}^d\) and momenta \(p \in \mathbb{R}^d\). The evolution of \(z:=[q \quad p]^T\) follows Newton’s laws.</p>

\[\begin{align}
    \label{eq:newtonslaws}
    \frac{d}{dt}p(t) = -\nabla_{q} U(q(t)) \\ 
    \frac{d}{dt}q(t) = p(t)
\end{align}\]

<p>It is often the case in experiments that, one can accurately record observables such as \(q\) (and sometimes \(p\)). Given \(q \textrm{ or } p\), we ask ourselves how accurately one can infer \(U\)? This is equivalent to asking, How accurately can one solve the following optimization problem,</p>

\[\begin{align}
    \label{eq:opt_prob}
    \arg \min_{\phi \in \mathcal{H}} \left\|\frac{d}{dt}p(t) + \nabla_{q} \phi(q(t))\right\|_{\mu} 
\end{align}\]

<p>in a hypothesis space \(\mathcal{H}\) of potential functions \(\phi\) with respect to some measure \(\mu\).</p>

<p>Assuming that the chosen measure \(\mu\) leads to a smooth loss landspace and our optimizer converges to the global minimum (say SGD), the accuracy of the inferred potential \(\phi\), depends on the “goodness” of \(\mathcal{H}\). (We will later clarify what goodness could mean.) Therefore the goal of this work is to find good hypothesis spaces for some commonly recurring interaction problems in computational chemistry.</p>

<h4 id="physical-systems-of-interest">Physical Systems of interest</h4>

<p>We investigate the following systems; ordered with respect to increasing computational complexity.</p>

<ol>
  <li>Gas in an infinite enclosure (homogenous / heterogenous)</li>
  <li>Gas in a finite enclosure with fixed walls (homogenous / heterogenous)</li>
  <li>Gas in a finite enclosure with one moving wall (homogenous / heterogenous)</li>
  <li>Nucleation of water droplet</li>
  <li>Solvation of Lysozyme in Water</li>
  <li>Homogeneous nucleation of water and carbon dioxide in carrier gas</li>
</ol>

<p>A collection of descriptions to these systems and their computational models can be found <a href="https://github.com/dynamic-queries/MolecularDynamicsZoo">here</a>.</p>

<h2 id="inference-models">Inference models</h2>]]></content><author><name></name></author><category term="research" /><category term="QChem" /><summary type="html"><![CDATA[Explainable force models in Computational Chemistry.]]></summary></entry></feed>