<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://dynamic-queries.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://dynamic-queries.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-08-04T17:16:00+00:00</updated><id>https://dynamic-queries.github.io/feed.xml</id><title type="html">Rahul Manavalan</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Physics informed Gaussian processes.</title><link href="https://dynamic-queries.github.io/blog/2023/pigp/" rel="alternate" type="text/html" title="Physics informed Gaussian processes." /><published>2023-08-01T00:00:00+00:00</published><updated>2023-08-01T00:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/pigp</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/pigp/"><![CDATA[<p><em>This is preliminary research conducted during my Masters studies (2020-2022) at TUM with <a href="https://www.cs.cit.tum.de/en/sccs/people/kislaya-ravi/">Ravi Kislaya</a></em>.</p>

<h3 id="introduction">Introduction</h3>

<p>Building on the previous post of inferring dynamical systems with <a href="https://dynamic-queries.github.io/blog/2023/mogp/">multi-output GPs</a>, we formulate learning a statistical model from data based on the structure of the mechanistic models underlying that dynamical system. To wit:</p>

<p>Consider the dynamical system.
\(\begin{align}
    \frac{dx(t)}{dt} = f(x(t)) \\ 
    x(0) = x_0 \in \mathbb{R}^d
\end{align}\)</p>

<p>If one has observables of this dynamical systems obtained at discrete time points such as \(X := [\hat{x}(t_1),\hat{x}(t_2),...,\hat{x}(t_N)]\) and the forcing function \(f(x)\), we would like to infer a statistical model (in this case, a GP) that aids as a reliable predictor for \(x(t)\). Here, GP is used as a surrogate and does not exactly fit the label of systems identification.</p>

<h3 id="method">Method</h3>

<p>This can be realized as follows. Let</p>

\[x(t) \sim \mathcal{GP}(\mu(t), \kappa(t,t'))\]

<p>Let \(\kappa\) be a sufficiently smooth covariance kernel. Then,</p>

\[\partial_t \: x(t) \sim \mathcal{GP}(\partial_t \mu(t), \partial_t \partial_{t'} \: \kappa(t,t'))\]

<p>From this, the log-likelihood can be written as</p>

\[log \: p(x|\theta) := \sum_{i=1}^N \: \| \partial_t \: x(t_i) - f(x(t_i))\|_2 + \|\hat{x}(t_i) - x_i(t_i) \|_2\]

<p>And as always one, chooses a zero mean Gaussian prior for a GP.</p>

\[p(\theta) := \mathcal{N}(0,\kappa(t,t'))\]

<p>There are a couple of approaches to this inference procedure. One could write down an expression for the posterior and perform MAP. Otherwise, one can do variational inference. We demonstrate both these procedures in the following example.</p>

<h3 id="examples">Examples</h3>

<h5 id="logistic-equation">Logistic equation</h5>

<h5 id="lotka-voltera-model">Lotka-Voltera model</h5>

<h5 id="duffling-oscillator">Duffling oscillator</h5>

<h5 id="robertsons-reaction-system">Robertson’s reaction system</h5>

<h3 id="relationship-with-probabilistic-ode-solvers">Relationship with probabilistic ODE solvers</h3>

<p>Probabilistic ODE solvers use the same method, but for the purpose of solving ODEs. However, when GPs are used as surrogates for finely discretized solutions to partial differential equations, incorporating the derivative information into the likelihood should result in improved inference, requiring fewer input reference datapoints. While the difference is subtle, it did not seem appropriate to further investigate this, considering that a large body of work had already been carried out on improving probabilistic integrators.</p>

<h3 id="code">Code</h3>]]></content><author><name></name></author><category term="projects" /><category term="SDEs" /><summary type="html"><![CDATA[Structure aware identification of dynamical systems with GPs.]]></summary></entry><entry><title type="html">A Langevin SDE simulator</title><link href="https://dynamic-queries.github.io/blog/2023/sde/" rel="alternate" type="text/html" title="A Langevin SDE simulator" /><published>2023-08-01T00:00:00+00:00</published><updated>2023-08-01T00:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/sde</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/sde/"><![CDATA[<p><em>This is an implementation of a Langevin dynamics simulator developed as part of my masters thesis. All figures, code are a work of my own.</em></p>

<h3 id="introduction">Introduction</h3>

<p>Consider a physical system \(\mathcal{S}\) with \(N\) molecules. Each of this molecule can occupy some portion of the Euclidian space \(q \in \Omega \subset \mathbb{R}^{3N}\) and have a velocity  \(p \in \mathcal{T}\Omega \subset \mathbb{R}^{3N}\). These quantities are sometimes called the canonical coordinates of \(\mathcal{S}\). It is often true, that one is interested more in the average properties of this molecular system, such as the temperature, pressure, enthalpy; and less in the actual trajectories of these molecules. It suffices to say, if there exists a probability distribution over \(\Omega \times \mathcal{T}\Omega\), say \(P(q,p)\) then any macroscopic property of interest \(\Phi\), can be obtained from the following integral</p>

\[\begin{align}
    \Phi := \mathbb{E}\left[\phi(q)\right] = \int P(q,p) \: \phi(q) \: dp \: dq 
\end{align}\]

<p>where \(\phi(q)\) is a constitutive relation, that we know from prior experience. (More or less)</p>

<p>But the question of how one arrives at \(P(q,p)\) still remains to be answered. Typically, this is done by sampling sufficiently many realizations from this distribution using some Monte Carlo scheme. Solving the Langevin SDE leads to one such Monte Carlo method, whose trajectories sufficently samples \(P(q,p)\). In its most generic form, the Langevin equation is as follows.</p>

\[\begin{align}
        d \begin{bmatrix} q \\ p \end{bmatrix} = \begin{bmatrix} M^{-1}p \: dt \\ -\nabla_q U(q) \: dt- \gamma p \: dt+ \sigma M^{-1/2} \: dW(t) \end{bmatrix}
    \end{align}\]

<p>\(U : q \mapsto \mathbb{R}\) is the potential energy function. \(M, \gamma, \sigma\) are parameters used in tuning the simulation. \(W(t)\) is a Wiener process. Sometimes referred to as Brownian motion; for \(t,s,k,l \in [0,T]\) a Wiener process is defined as follows:</p>

\[\begin{align}
        W(0) = 0 \\ 
        \forall s&gt;t \quad W(s)-W(t) \sim \sqrt{s-t}\:\mathcal{N}(0,1) \\ 
        W(s)-W(t) \textrm{ and } W(l)-W(k) \textrm{ are indepenedent of each other.}
    \end{align}\]

<p>For most practical purposes the Langevin SDE needs to be numerically simulated. This entails making time discretizations. It is usually true that the equations are stiff and small time discretization steps need to be taken for accurately simulating \(P(q,p)\). On the other hand, it could also be that smaller time steps, would require significantly longer computational runs before the entire distribution can be sampled. This “chicken and egg” dilemma, presents the need for numerical integrators that can efficiently simulate the Langevin equation without compromising accuracy.</p>

<p>This article discusses some of these numerical schemes, provides implementations of these methods and illustrates them on the classical harmonic and double well potentials.</p>

<h3 id="operator-splitting">Operator splitting</h3>

<p>Operator splitting is a numerical approximation technique, where the vector field of an ordinary differential equation is complicated, but can be decomposed into an additive sum of terms that are trivial when treated individually. In the context of the Langevin SDE, this is</p>

\[\begin{align}
        d \begin{bmatrix} q \\ p \end{bmatrix} = \begin{bmatrix} M^{-1}p \\ 0 \end{bmatrix} \: dt  + \begin{bmatrix} 0 \\ -\nabla_q U(q)  \end{bmatrix} \: dt + \begin{bmatrix} 0 \\  -\gamma p \: dt + \sigma M^{-1/2} \: dW(t) \end{bmatrix}
    \end{align}\]

<p>Here, each of the terms on the right hand side, is tractable – some analytically and the one with \(U\) numerically.</p>

<hr />

<p>Formally let the solution to</p>

\[\begin{align}
    d \begin{bmatrix} q \\ p \end{bmatrix} = \begin{bmatrix} M^{-1}p \\ 0 \end{bmatrix} \: dt
\end{align}\]

<p>be</p>

\[\begin{align}
    \phi_A(t) := \begin{bmatrix} q(t) \\ p(t) \end{bmatrix}_A = \begin{bmatrix} e^{M^{-1}t}q(0) \\ p(0) \end{bmatrix}
\end{align}\]

<p>And the solution to</p>

\[\begin{align}
    d \begin{bmatrix} q \\ p \end{bmatrix} = \begin{bmatrix} 0  \\ -\gamma p \: dt + \sigma M^{-1/2} \: dW(t)\end{bmatrix}
\end{align}\]

<p>be</p>

\[\begin{align}
    \phi_O(t) := \begin{bmatrix} q(t) \\ p(t) \end{bmatrix}_O = \begin{bmatrix} q(0) \\ e^{-\gamma t} p(0) + \frac{\sigma}{\sqrt{2 \gamma}} \sqrt{1-e^{-2\gamma t}} M^{-1/2} R(t)\end{bmatrix}
\end{align}\]

<p>where \(R(t) \sim \mathcal{N}(0,1)\).</p>

<p>Finally, there exists no analytical solution to</p>

\[\begin{align}
    d \begin{bmatrix} q \\ p \end{bmatrix} = \begin{bmatrix} 0  \\ -\nabla_q U(q)\end{bmatrix} \: dt
\end{align}\]

<p>in the general case and needs to be simulated numerically. We call this solution</p>

\[\begin{align}
    \phi_B(t) := \begin{bmatrix} q(t) \\ p(t) \end{bmatrix}_B
\end{align}\]

<hr />

<p>The idea behind operator splitting is to compose these individual solutions in a specific sequence that results in minimum error. For this Langevin problem, <a href="https://arxiv.org/abs/1304.3269">Leimkuhler</a> considered several sequences. We will discuss two of them.</p>

<h5 id="baoab">BAOAB</h5>

\[\begin{align}
    \phi(t+\tau) := \phi_B \left(t+\frac{\tau}{2}\right) \circ \phi_A \left(t+\frac{\tau}{2}\right) \circ \phi_O(t + \tau) \circ \phi_A \left(t+\frac{\tau}{2}\right) \circ \phi_B \left(t+\frac{\tau}{2}\right)
\end{align}\]

<h5 id="aboba">ABOBA</h5>

\[\begin{align}
    \phi(t+\tau) := \phi_A \left(t+\frac{\tau}{2}\right) \circ \phi_B \left(t+\frac{\tau}{2}\right) \circ \phi_O(t + \tau) \circ \phi_B \left(t+\frac{\tau}{2}\right) \circ \phi_A \left(t+\frac{\tau}{2}\right)
\end{align}\]

<h3 id="examples">Examples</h3>

<p>In the following, we consider one dimensional molecular systems. That is \(q,p \in \mathbb{R}^N\). With these assumptions, the simulated trajectories and their distributions, along with the energies are shown below.</p>

<h4 id="one-body-system">One body system</h4>
<p>Here \(N=1\)</p>
<h5 id="simple-harmonic-oscillator">Simple Harmonic oscillator</h5>

\[\begin{align}
    U(q) = \frac{1}{2} k(q-c)^2
\end{align}\]

<h5 id="double-well-potential-oscillator">Double Well potential oscillator</h5>

\[\begin{align}
    U(q) = \frac{1}{4} k(q-a)^2(q+a)^2
\end{align}\]

<h4 id="two-body-systems">Two body systems</h4>

<p>Here \(N=2\), that is \(q := \begin{bmatrix} q_1 \\ q_2 \end{bmatrix}\)</p>

<h5 id="double-well-with-pairwise-harmonic-potential">Double well with pairwise harmonic potential</h5>

\[\begin{align}
    U(q_1) = \frac{1}{4} k(q_1-a)^2(q_1-b)^2 + \frac{1}{2} k(q_1-q_2)^2
\end{align}\]

<h5 id="double-well-with-soft-bound-potential">Double well with soft-bound potential</h5>

\[\begin{align}
    U(q_1) = \frac{1}{4} k(q_1-a)^2(q_1-b)^2 + V(q_1, q_2) \\ 
    V(q_1, q_2) = \begin{cases} cos\left(1 + \frac{ (q_2 - q_1) \pi }{r_c}\right) \quad r&lt;r_c \\ 0 \quad r \geq r_c \end{cases}
\end{align}\]

<p>Extending these simulations to higher dimensions is trivial, but remains out of the scope of this article.</p>]]></content><author><name></name></author><category term="education" /><category term="SDEs" /><summary type="html"><![CDATA[With emphasis on numerical methods.]]></summary></entry><entry><title type="html">(Original Research) Random feature potentials</title><link href="https://dynamic-queries.github.io/blog/2023/random_models/" rel="alternate" type="text/html" title="(Original Research) Random feature potentials" /><published>2023-07-22T08:00:00+00:00</published><updated>2023-07-22T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/random_models</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/random_models/"><![CDATA[<p>This is original work done as a part of my masters studies at TUM with <a href="http://christian.mendl.net/">Chrisitian Mendl</a> and <a href="https://fd-research.com/">Felix Dietrich</a>.</p>

<p><em>All ideas, figures, equations, examples and code, unless cited, are a work of my own.</em></p>

<h3 id="introduction">Introduction</h3>

<p>The importance of understanding the composition and behaviour of molecules is well established. Scientific disciplines such as chemistry, molecular biology, drug discovery and others, rely on molecular dynamics simulations as their core computational tool. 
Tradionally, MD has been and to some extent still is, dependent on empirical force fields for estimating the interaction forces among molecules.</p>

<p>These empirical models are a simplification at best. The “proper” way to evaluate the forces in a molecular dynamics simulation is to use first principle calculations that solves the stationary Schrodinger’s equations. But self-consistent loop sub-routines in most first principle calculations are too expensive to simulate molecules realistically for time domains where phenomena of interest emerge.</p>

<p>This brings us to the fore of research today, where machine learning algorithms (deep learning, especially) has been co-opted to “learn” the rules of quantum mechanics from huge clusters of molecular data, generated from first principle calculations. The models used are the usual suspects – neural networks, gaussian processes and other kernel methods.</p>

<p>In this work, we investigate the efficacy of random feature models as suitable surrogates for learning the ground state potentials of molecular systems.</p>

<p><em>I am aware that these two topics are esoteric in their own right. Therefore, I provide a quick interlude that discusses the notions of 1) Potential energy surfaces of molecular systems and 2) Random feature models as function approximation tools.</em></p>

<h3 id="interlude">Interlude</h3>

<h4 id="potential-energy-surfaces">Potential energy surfaces</h4>

<h4 id="random-feature-models">Random feature models</h4>
<p>Consider a sufficiently regular function \(f:x \mapsto y\). Then one can find bases functions \(\phi_i(x; \theta)\) with parameteric dependence on \(\theta\) such that</p>

\[\begin{align}
    f(x) \approx \sum_i a_i \phi_i(x; \theta)
\end{align}\]

<p>This is a common feature in most function approximation schemes. In particular, if one uses a neural network with one hidden layer:</p>

\[\begin{align}
    \label{eq:randomNN}
    f(x) \approx W_2 \: \rho \left(W_1 x  + b_1 \right)
\end{align}\]

<p>Tradionally, one randomly chooses the parameters which are subsequently trained with a gradient based algorithm. If one assumes that the weights \(\theta= \{W_1, W_2, b_1\}\) follow a probability distribution \(p_{\theta}\), during training this distribution is modified with every epoch.</p>

<p>Random feature models, on the otherhand,  sample \(\theta \sim \mathcal{U}_{\theta}\) from a uniform distribution (which requires no training). As surprising it is, it has been shown with uniform sampling of \(\theta\) in (\ref{eq:randomNN}), used in conjunction with \(\rho = \{cos, tanh, relu\}\) is a universal approximator. Further, one can define a litany of heuristics to define ad-hoc “probability” functions \(p_i\), which when used for sampling should yield more accurate approximations at the same cost.</p>

<p>Shown below is the function \(x \mapsto sin(x)\) trained with an ADAM optimizer and one sampled using the sampling algorithm (I will discuss this next.). The prediction from the sampling algorithm is one shot, requiring no iterative training training, that is characteristic of all gradient based training methods.</p>

<style>
    .column {
  float: left;
  width: 50.00%;
  margin : 0 0 0px 0px;
  padding: 2px;
}

/* Clear floats after image containers */
.row::after {
  content: "";
  clear: both;
  display: table;
}
</style>

<div class="row">
  <div class="column">
    <img style="width:100%" src="/assets/random_feature/single_output.svg" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:100%" src="/assets/random_feature/single_sampling.svg" alt="spline-sur" />
  </div>
</div>

<p>This is also the case for a multioutput function such as</p>

\[x \mapsto \begin{bmatrix} sin(4x) \\ cos(4x) \\ sin(4x) \: cos(4x)\end{bmatrix}\]

<div class="row">
  <div class="column">
    <img style="width:100%" src="/assets/random_feature/multi_output.svg" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:100%" src="/assets/random_feature/multi_sampling.svg" alt="spline-sur" />
  </div>
</div>

<h3 id="code">Code</h3>

<h5 id="random-feature-model-for-approximating-simple-functions">Random feature model for approximating simple functions</h5>

<script src="https://gist.github.com/dynamic-queries/7fe5162d9f355fe9e7414cf50a8fdfa0.js"></script>]]></content><author><name></name></author><category term="projects" /><category term="Surrogatization" /><summary type="html"><![CDATA[Inferring potential energy surfaces with random feature models.]]></summary></entry><entry><title type="html">(Original Research) Bayesian Waveform Inversion</title><link href="https://dynamic-queries.github.io/blog/2023/bayesian_inversion/" rel="alternate" type="text/html" title="(Original Research) Bayesian Waveform Inversion" /><published>2023-07-21T08:00:00+00:00</published><updated>2023-07-21T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/bayesian_inversion</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/bayesian_inversion/"><![CDATA[<p>This is original work done as a part of a student research project during my masters studies at TUM with <a href="https://fd-research.com/">Felix Dietrich</a>.</p>

<p><em>All ideas, figures, equations, examples and code, unless cited are a work of my own.</em></p>

<h3 id="introduction">Introduction</h3>

<p><strong>Waveform inversion</strong> is principally parameter estimation with three insights.</p>

<ol>
  <li>The parameter is a field over some domain \(\Omega\).</li>
  <li>Only partial observations of the quantity of interest are available.</li>
  <li>Observables are obtained from evaluating a family of wave propagation models.</li>
</ol>

<p>Consider a simple example of an acoustic wave equation.</p>

\[\begin{align}
    x \in \Omega \subset \mathbb{R} \\ 
    t \in \mathbb{R}^+ \\ 
    \frac{\partial}{\partial t}  u(x,t) = \frac{\partial^2}{\partial x^2} \left[c(x)^2 u(x,t)\right] \\ 
    u(x,0) = f(x) \\ 
    \frac{\partial }{\partial t}u(x,0) = g(x) \\
    u(y,t) = 0 \quad \forall y \in \delta \Omega
\end{align}\]

<p>Here \(f,g\) are suitably chosen initial conditions that would sustain elastic progation of a wave in \(\Omega\). In the context of waveform inversion, \(c(x)\) is the parameter and \(u(z,t) \: \forall z \in \hat{\Omega} \subset \Omega\) is the observable. For the sake of this article, consider the following example for \(c,u\).</p>

<style>
    .column {
  float: left;
  width: 50.00%;
  margin : 0 0 0px 0px;
  padding: 2px;
}

/* Clear floats after image containers */
.row::after {
  content: "";
  clear: both;
  display: table;
}
</style>

<div class="row">
  <div class="column">
    <img style="width:100%" src="/assets/bayesian/parameter.svg" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:100%" src="/assets/bayesian/observable.svg" alt="spline-sur" />
  </div>
</div>

<p>The goal is to reconstruct \(c(x)\) (<em>to the left</em>) from \(u(z,t)\) (<em>to the right</em>) measured at \(z=\{z_0, z_i\}\) where \(z_i \in \Omega\). The optimal \(c^{*}(x)\) is the minimizer to</p>

\[\begin{align}
  \label{eq:opt}
  \arg \min_{\hat{c} \in \mathcal{H}} \sum_{i}\| \hat{u}(z_i,t; \hat{c}(x)) - u(z_i,t)\|
\end{align}\]

<p>where \(\hat{u}(z_0,t)\) is evaluated by solving the acoustic wave equation using a robust finite difference method <em>(see code)</em>.</p>

<h3 id="parameter-estimation">Parameter estimation</h3>
<p>Estimating \(c(x)\) is non-trivial. Firstly, one has to choose an appropriate bases to parameterize this function. This of course varies on a case by case basis. For the example above, radial bases functions are good approximants. Therefore,</p>

\[\begin{align}
  c(x) = \sum_{i=1}^{N} a_i \phi_i (x) \\ 
  \phi_i(x) := \phi(x;x_i,\mu) = \exp\left[{-\left(\frac{x-x_0}{\mu}\right)^2}\right] \\ 
  N = 5
\end{align}\]

<p>Therefore the parameterization space is now \(\mathcal{A} \ni a\).</p>

<p>\(N\) is, by design, chosen sufficiently small to faciliate comparison with Bayesian optimization.</p>

<h4 id="frequentist-estimation">Frequentist estimation</h4>
<p>Problem (\ref{eq:opt}) is ill-posed in the sense of Hadamard. As a result, it is initially solved with tradional gradient based optimization algorithms with suitable regularization. The algorithms used in this pursuit, are briefy outlined and the “best possible” reconstructions are presented.</p>

<p>The reconstructions here, must be taken with a grain of salt, for it is entirely possible to come up with an excellent initial guess that could converge to the actual \(c(x)\) accurately.</p>

<p>Furthermore, it is observed that the reconstructions are sensitive to the choice of the type of regularization and the magnitude of the regularization constant. Common regularizations to an \(l_2\) loss includes weighted norms of inputs, finite difference of \(l_2\) loss with respect to time, fourier coefficients of \(l_2\) loss and so on.</p>

<p>In the presented cases, \(l_2\) Tikhonov regularization is used. That is</p>

\[\begin{align}
  \arg \min_{a \in \mathcal{A}} \sum_{i}\| \hat{u}(z_i,t; \hat{c}(x;a)) - u(z_i,t)\| + \lambda \| \hat{c}(x;a) \|
\end{align}\]

<p>The values of the regularization constants and the number of iterations used for each of these algorithms is indicated below.</p>

<h5 id="adam">ADAM</h5>

<p>This is a first order optimization algorithm introduced in 2014 by <a href="https://arxiv.org/abs/1412.6980">Kingma</a>. It is a variant of momentum based methods such as Adagrad and Nesterov optimization methods. Generically, if \(f_{\theta}\) is the objective function that needs to be optimized with respect to the parameters \(\theta\), it is done so as follows.</p>

<hr />
<p><strong><em>Algorithm</em></strong></p>

<p>Intitialize:</p>
<ol>
  <li>Constants \(\beta_1 = 0.9, \beta_2 = 0.999, \alpha = 0.001 , \epsilon=10^{-8}\)</li>
  <li>Initial positions and velocities \(m_0 = 0, v_0 = 0\)</li>
  <li>Make a “good initial guess” \(\theta_0\)</li>
</ol>

<p>For the \(t^{th}\) iteration:</p>

<ol>
  <li>Evaluate the gradient of the function \(g_t := \nabla_{\theta} f_{\theta_{t-1}}\)</li>
  <li>Compute “position” term : \(m_t = \frac{\beta_1}{1 - \beta_1 ^ t} m_{t-1} + \frac{1-\beta_1}{1 - \beta_1 ^ t} g_t\)</li>
  <li>Compute “momentum” term : \(v_t = \frac{\beta_2}{1 - \beta_2 ^ t} v_{t-1} + \frac{1-\beta_2}{1 - \beta_2 ^ t} g_t^2\)</li>
  <li>Update parameters : \(\theta_t = \theta_{t-1} - \alpha \frac{m_t}{\sqrt{v_t} + \epsilon}\)</li>
</ol>

<p>For \(t \in [1,\textrm{maxiters}] \subset \mathbb{N}\)</p>

<ol>
  <li>Repeat the four previous steps.</li>
</ol>

<hr />

<p>Using the previous algorithm and with a regularization constant \(\lambda = 2.01 \pm 10^{-4}\), one obtains the following reconstruction.</p>

<p><img style="width:60%" src="/assets/bayesian/recon_parameter_adam.svg" alt="spline-sur" /></p>

<h5 id="bfgs">BFGS</h5>

<p>One can also use, higher order optimization methods that make use of  \(\nabla_{\theta}(\nabla_{\theta} f_{\theta})\). Alas, computing the Hessian for every iteration is computationally expensive.
<a href="https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm">BFGS method</a> is a quasi-Netwon method which instead of computing the entire Hessian, approximates it with several evalutions of the gradient of \(f_{\theta}\).</p>

<p>Akin, to the previous section, one obtains a minimizer \(\theta^*\) for \(f_{\theta}\) as follows.</p>

<hr />
<p><strong><em>Algorithm</em></strong></p>

<p>Initialize:</p>

<ol>
  <li>Initial Hessian approximant and initial learning rate : \(B_0 = \mathcal{I}, \alpha_0 = 10^{-2}\)</li>
  <li>Make a “good initial guess” \(\theta_0\)</li>
</ol>

<p>For every \(k^{th}\) iteration:</p>

<ol>
  <li>Estimate new search direction, \(p_k\) by solving \(B_{k-1} p_k = -\nabla_{\theta} f_{\theta_{k-1}}\)</li>
  <li>Estimate step size, \(\alpha_k\) by solving \(\arg \min_{\alpha_k} f_{\theta}(\theta_{k-1} + \alpha_k p_k)\)</li>
  <li>Compute new direction \(\theta_{k} = \theta_{k-1} + \alpha_k p_k\)</li>
  <li>Estimate update vector \(y_k = \nabla_{\theta} f(\theta_{k}) - \nabla_{\theta} f(\theta_{k-1})\)</li>
  <li>Let \(s_k = \alpha_k p_k\)</li>
  <li>Update Hessian approximant \(B_{k} = B_{k-1} + \frac{y_k y_k^T}{y_k^T s_k} + \frac{B_{k-1} s_k^T s_k B_{k-1}^T}{s_k^T B_{k-1} s_k}\)</li>
</ol>

<p>For \(k \in [1,\textrm{maxiters}] \subset \mathbb{N}\)</p>

<ol>
  <li>Repeat the six previous steps.</li>
</ol>

<hr />

<p>Using the previous algorithm and with a regularization constant \(\lambda = 0.399 \pm 10^{-3}\), one obtains the following reconstruction.</p>

<p><img style="width:60%" src="/assets/bayesian/recon_parameter_bfgs.svg" alt="spline-sur" /></p>

<h5 id="issues">Issues</h5>

<p>There are two difficulties that one encounters with the frequentist estimation while treating an ill-conditioned problem such as this one.</p>

<ol>
  <li>Good initial guesses are vital for fast convergence.</li>
  <li>Estimation is very sensitive to the value of \(\lambda\).</li>
</ol>

<p>Elaborating on (2), consider the following reconstructions with BFGS optimization for \(\lambda = 0.397,\:0.399\:,0.40\:,0.402\)  respectively.</p>

<div class="row">
  <div class="column">
    <img style="width:80%" src="/assets/bayesian/bfgs_0.397.svg" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:80%" src="/assets/bayesian/bfgs_0.399.svg" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:80%" src="/assets/bayesian/bfgs_0.4.svg" alt="spline-sur" />
  </div>
  <div class="column">
    <img style="width:80%" src="/assets/bayesian/bfgs_0.402.svg" alt="spline-sur" />
  </div>
</div>

<p>Such sensitivity makes any practical grid search (which is performed at a much higher resolution) useless, questioning the practicality of such optimization routines for practical applications.</p>

<h4 id="bayesian-estimation">Bayesian estimation</h4>
<p>Plagued by the ill-posedness of frequentist estimation, it seems appropriate to turn to Bayesian estimation. Granted, one does need a good prior for Bayesian methods to make computational sense, however Bayesian inverse problems are well posed (As highlighted in <a href="https://www.cambridge.org/core/journals/acta-numerica/article/abs/inverse-problems-a-bayesian-perspective/587A3A0D480A1A7C2B1B284BCEDF7E23">Stuart</a>).</p>

<p>Appropriately, the Bayes rule for this inference problem can be written as</p>

\[\begin{align}
  p(a | u(z)) = \frac{p(u(z)|a)\:p(a)}{p(u(z))}
\end{align}\]

<p>It is known that, esimating the marginal density \(p(u(z))\) is computationlly intractable. Therefore, in what follows two classes of Bayesian state estimation methods – MCMC and HMC (and its variants) are used to estimate \(p(a|u(z))\) from which one can compute the expectation, variance and other higher order moments of \(\hat{c}(x;a)\).</p>

<h5 id="markov-chain-monte-carlo">Markov Chain Monte carlo</h5>

<h6 id="metropolis-hasting-sampling">Metropolis Hasting sampling</h6>

<h6 id="gibbs-sampling">Gibbs sampling</h6>

<h5 id="hamiltonian-monte-carlo">Hamiltonian Monte carlo</h5>

<h6 id="plain-hmc">Plain HMC</h6>

<h6 id="no-u-turn-sampling-hmc">No U-turn sampling HMC</h6>

<h3 id="code">Code</h3>

<p>These results can be reproduced with the following code. <em>(Documentation coming soon.)</em></p>

<script src="https://gist.github.com/dynamic-queries/3473ab7b87744cd1ad07cddd74eabd03.js"></script>]]></content><author><name></name></author><category term="projects" /><category term="InverseProblems" /><summary type="html"><![CDATA[Full waveform inversion with MCMC, HMC, NUTS]]></summary></entry><entry><title type="html">(Original Research) Latent space Bayesian Waveform Inversion</title><link href="https://dynamic-queries.github.io/blog/2023/latent_space_bayesian_FWI/" rel="alternate" type="text/html" title="(Original Research) Latent space Bayesian Waveform Inversion" /><published>2023-07-21T08:00:00+00:00</published><updated>2023-07-21T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/latent_space_bayesian_FWI</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/latent_space_bayesian_FWI/"><![CDATA[<h3 id="introduction">Introduction</h3>
<p>Waveform inversion concerns reconstructing velocity fields from remotely measured wave signals. It has been common practice to optimize for velocity fields parameterized as indicator functions. Considering the ill-posed nature of inversion, the trend is to turn to Bayesian inversion methods, which can be formulated as well posed problems. In addition, Bayesian methods provides one, an estimate of the uncertainity in prediction, which is ever so crucial, should waveform inversion be used for resource sensitive applications.</p>

<p>Despite all evidences pointing towards adopting Bayesian parameter estimation for waveform inversion, it is still not common practice in the FWI community; largely because Bayesian methods are extremely expensive for models with large parameters.</p>

<p>In this work, we use the time honored practice of seeking latent space representations for families of velocity fields. Subsequently we demonstrate that Bayesian waveform inversion in these latent spaces becomes computational tractable.</p>

<h3 id="latent-spaces">Latent spaces</h3>

<p>We begin with some observations. More specifically, we consider some velocity fields that are commonplace in the inversion literature – seismic and otherwise.</p>

<h4 id="fourier-space-representations">Fourier space representations</h4>
<p>Initially, we seek Fourier space latent representation of the fields. Initial observations indicate that one can do away with more than 80% of the Fourier bases functions and still obtain a qualitatively good estimate of the velocity field at the cost of mapping to and back from the Fourier space. (This is realized with <strong>fft</strong>.)</p>

<h5 id="primitives">Primitives</h5>

<style>
    .column {
  float: left;
  width: 50.00%;
  margin : 0 0 0px 0px;
  padding: 20px;
}

/* Clear floats after image containers */
.row::after {
  content: "";
  clear: both;
  display: table;
}
</style>

<div class="row">
  <div class="column">
    <img style="width:100%" src="/assets/latent/circle.svg" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:100%" src="/assets/latent/square.svg" alt="spline-sur" />
  </div>
</div>

<div class="row">
   <div class="column">
    <img style="width:100%" src="/assets/latent/2circle.svg" alt="spline-sur" />
    </div>
   <div class="column">
    <img style="width:100%" src="/assets/latent/circle4.svg" alt="spline-sur" />
  </div>
</div>

<h5 id="velocity-classes">Velocity classes</h5>

<div class="row">
  <div class="column">
    <img style="width:100%" src="/assets/latent/simple.svg" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:100%" src="/assets/latent/complex.svg" alt="spline-sur" />
  </div>
</div>

<div class="row">
   <div class="column">
    <img style="width:100%" src="/assets/latent/Csimple.svg" alt="spline-sur" />
    </div>
   <div class="column">
    <img style="width:100%" src="/assets/latent/Ccomplex.svg" alt="spline-sur" />
  </div>
</div>

<p>One should note that, for discontinuous functions, high frequency Fourier bases functions become increasingly relevant. While, this was not so debilitating for the examples in the Primitive section, it is clear that seismic velocity fields cannot be robustly parameterized.</p>

<h4 id="learned-latent-space-representations">Learned latent space representations</h4>

<p>This leads us to learning the latent space representations using <a href="https://arxiv.org/abs/1906.02691">variational autoencoders</a>.</p>

<h5 id="autoencoders">Autoencoders</h5>

<p>But let’s initially review autoencoders. Autoencoders are approximation methods for learning a latent space representation \(z\) for a given vector \(x\) in the observable space. The latent space representation is sometimes called an embedding. A generic autoencoder consists of – an encoder and a decoder (as shown below).</p>

<style>
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}
</style>

<p><img style="width:70%" class="center" src="/assets/latent/AE/outline.svg" alt="spline-sur" /></p>

<p>Any \(x\) is hierarchically projected into a lower dimensional space upto the latent space using a dense fully connected neural network. This can also be realized using other network architectures such as CNNs and GNNs which can incorporate more inductive bias. The decoder is the mirror image of an encoder and aids in the reconstruction \(z \mapsto x\).</p>

<p>For instance, a reconstruction of the MNIST handwritten images, by sampling from the latent space and reconstructing it back, is shown below. Here \(x \in \mathbb{R}^{28 \times 28}\) and \(z \in \mathbb{R}^2\). Coincidentally, the MNIST digits can be viewed as spline shaped defects embedded in the domain of interest (similar to the primitives in the Fourier representations section.)</p>
<style>
    .columnM {
  float: left;
  width: 25.00%;
  margin : 0 0 0px 0px;
  padding: 0px;
}

/* Clear floats after image containers */
.rowM::after {
  content: "";
  clear: both;
  display: table;
}
</style>

<div class="row">
  <div class="columnM">
    <img style="width:100%" src="/assets/latent/AE/0.png" alt="spline-sim" />
  </div>
  <div class="columnM">
    <img style="width:100%" src="/assets/latent/AE/1.png" alt="spline-sur" />
  </div>
  <div class="columnM">
    <img style="width:100%" src="/assets/latent/AE/2.png" alt="spline-sim" />
  </div>
  <div class="columnM">
    <img style="width:100%" src="/assets/latent/AE/3.png" alt="spline-sur" />
  </div>
  <div class="columnM">
    <img style="width:100%" src="/assets/latent/AE/4.png" alt="spline-sim" />
  </div>
  <div class="columnM">
    <img style="width:100%" src="/assets/latent/AE/5.png" alt="spline-sur" />
  </div>
  <div class="columnM">
    <img style="width:100%" src="/assets/latent/AE/6.png" alt="spline-sim" />
  </div>
  <div class="columnM">
    <img style="width:100%" src="/assets/latent/AE/7.png" alt="spline-sur" />
  </div>
  <div class="columnM">
    <img style="width:100%" src="/assets/latent/AE/8.png" alt="spline-sim" />
  </div>
  <div class="columnM">
    <img style="width:100%" src="/assets/latent/AE/9.png" alt="spline-sur" />
  </div>
</div>

<p>However, autoencoders are also notoriously hard to train; with gradient based optimization algorithms trapping themselves often in local minima.</p>

<div class="row">
  <div class="columnM">
    <img style="width:100%" src="/assets/latent/AE_VF/0.png" alt="spline-sim" />
  </div>
  <div class="columnM">
    <img style="width:100%" src="/assets/latent/AE_VF/1.png" alt="spline-sur" />
  </div>
  <div class="columnM">
    <img style="width:100%" src="/assets/latent/AE_VF/2.png" alt="spline-sim" />
  </div>
  <div class="columnM">
    <img style="width:100%" src="/assets/latent/AE_VF/3.png" alt="spline-sur" />
  </div>
  <div class="columnM">
    <img style="width:100%" src="/assets/latent/AE_VF/4.png" alt="spline-sim" />
  </div>
  <div class="columnM">
    <img style="width:100%" src="/assets/latent/AE_VF/5.png" alt="spline-sur" />
  </div>
  <div class="columnM">
    <img style="width:100%" src="/assets/latent/AE_VF/6.png" alt="spline-sim" />
  </div>
  <div class="columnM">
    <img style="width:100%" src="/assets/latent/AE_VF/7.png" alt="spline-sur" />
  </div>
  <div class="columnM">
    <img style="width:100%" src="/assets/latent/AE_VF/8.png" alt="spline-sim" />
  </div>
  <div class="columnM">
    <img style="width:100%" src="/assets/latent/AE_VF/9.png" alt="spline-sur" />
  </div>
</div>

<p>This is the case with the results above, where the embeddings cannot sufficiently represent the different velocity fields faithfully. In light of this, it makes more sense to look into VAEs which have been shown to be more robust to this issue.</p>

<h5 id="variational-autoencoders">Variational Autoencoders</h5>

<p>Variational Autoencoders (VAEs) approach this representation learning problem from a Bayesian perspective. It assumes that there exists a joint probability distribution \(p(x,z)\). Then for a prior over \(z, p(z) \sim \mathcal{N}(\mu, \Sigma)\), one can find predictors for \(x\) using Bayes rule. That is,</p>

\[\begin{align}
    p(z|x) = \frac{p(x|z)\:p(z)}{p(x)}
\end{align}\]

<p>The marginal \(p(x)\) is untractable for most practical scenarios. However, one can use variational inference to estimate the posterior \(p(z|x)\) with \(q_{\theta}(z|x)\). That is, one can optimize for the parameters \(\theta\) by minimizing the Kullback-Leiber (KL) divergence between the two distributions. The backward KL divergence in this case is given by</p>

\[\begin{align}
  KL \left[q(z|x) || p(z|x)\right] := \int dz \: q(z|x) \: log\left[\frac{q(z|x)}{p(z|x)}\right]
\end{align}\]

<p>This ofcourse is intractable, since it still contains the marginal \(p(x)\). Fortunately the above expression can be re-written as follows</p>

\[\begin{align}
    EUBO := \int dz \: q(z|x) \: \left[log\left[q(x,z)\right] - log\left[q(z|x)\right]\right]  = -\int dz \: q(z|x) \: log\left[p(x)\right] +   KL \left[q(z|x) || p(z|x)\right] 
\end{align}\]

<p>Minimizing the left equation on the left hand side, also minimzes the KL divergence, since</p>

\[\begin{align}
  \int dz \: q(z|x) \: log\left[p(x)\right] \geq -EUBO 
\end{align}\]

<p>This is the evidence upper bound or the EUBO, which is actually tractable. In the context of the autoencoder,</p>

<ol>
  <li>\(q(z|x)\) is the probability distribution over predictions from the encoder.</li>
  <li>\(p(x|z)\) is the same for the decoder.</li>
  <li>\(p(z) \sim \mathcal{N}(\mu, \Sigma)\) is the prior.</li>
</ol>

<p>The general architecture of a VAE is as follows. Here, the output of the encoder is the mean \(\mu\) and the covariance matrix \(\Sigma\) of a multi-variate normal distribution \(\mathcal{N}\). The samples from \(z \sim \mathcal{N}\) constitutes the latent space variable.</p>

<p><img style="width:70%" class="center" src="/assets/latent/vae_outline.svg" alt="spline-sur" /></p>

<p>Training the VAE comprises minimizing the EUBO, for which one needs to make a few assumptions.</p>]]></content><author><name></name></author><category term="projects" /><category term="InverseProblems" /><summary type="html"><![CDATA[Waveform inversion on parsimonious models]]></summary></entry><entry><title type="html">SINDY - Sparse Identification of non-linear dynamical systems.</title><link href="https://dynamic-queries.github.io/blog/2023/sindy/" rel="alternate" type="text/html" title="SINDY - Sparse Identification of non-linear dynamical systems." /><published>2023-07-20T08:00:00+00:00</published><updated>2023-07-20T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/sindy</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/sindy/"><![CDATA[<p>This work is a re-creation of a symbolic regression method from <a href="https://www.pnas.org/doi/10.1073/pnas.1517384113">Brunton and Kutz</a>. The article is meant to be a quick summary of this method.</p>

<p><em>All figures, equations, examples and code are a work of my own. Only the idea has been borrowed from the paper.</em></p>

<h3 id="introduction">Introduction</h3>

<p>Data driven methods are the modern day equivalent to Poincare’s work in the 19th century. A characteristic feature of these methods is determining the equations of motion underlying a given dynamical system, exclusively from time dependent observables, measured from that system.</p>

<hr />
<p>Consider this generic form of a dynamical system.</p>

\[\begin{align}
    \frac{dx(t)}{dt} = f(x(t)) \\ 
    x(0) = x_0 \in \mathbb{R}^d
\end{align}\]

<p>Given \(X := [x(t_1),x(t_2),...,x(t_N)]\), we would like to infer \(f\) either symbolically or numerically.</p>

<hr />

<h3 id="sindy">SINDY</h3>

<p>SINDY is a symbolic regression algorithm that identifies \(f\), provided one has access to the most apposite bases set, say \(\mathcal{B}\), that spans the function space \(\mathcal{F} \ni f\). It’s usually the case that for most systems, knowledge of \(\mathcal{B}\) is sparse at best; raising serious questions about the practicality of this method for data arising from real world applications. Nonetheless it is a nice method where one has expert knowledge about the system. The algorithm has four main steps.</p>

<hr />

<p><strong><em>Algorithm</em></strong></p>

<ol>
  <li>
    <p>Evaluate \(\dot{X} := [\frac{dx(t_1)}{dt},\frac{dx(t_2)}{dt},...,\frac{dx(t_N)}{dt}] \in \mathbb{R}^{d \times N}\) using a sufficiently robust finite difference scheme.</p>
  </li>
  <li>
    <p>Choose a bases set \(\mathcal{B} := \{b_1(x), b_2(x), ..., b_k(x)\}\) of functions.</p>
  </li>
  <li>
    <p>Evalute \(\Theta(X) := \begin{bmatrix} b_1(x(t_1)) &amp;&amp; ... &amp;&amp; b_1(x(t_N)) \\ \vdots &amp;&amp;  &amp;&amp; \vdots \\  b_k(x(t_1)) &amp;&amp; ... &amp;&amp; b_k(x(t_1)) \\ \end{bmatrix} \in \mathbb{R}^{k \times d \times N}\)</p>
  </li>
  <li>
    <p>If \(\dot{X} = K \: \Theta(X)\) where \(K \in \mathbb{R}^{k}\). Solve for \(K\) using linear least squares and a sparsity inducing regularizer.</p>
  </li>
</ol>

<hr />

<h3 id="examples">Examples</h3>

<p>Here the algorithm is applied to commonly recurring examples in literature – Lorenz 69 attractor and a Lotka-Volterra model.</p>

<style>
    .column {
  float: left;
  width: 50.00%;
  margin : 0 0 0px 0px;
  padding: 2px;
}

/* Clear floats after image containers */
.row::after {
  content: "";
  clear: both;
  display: table;
}
</style>

<h4 id="lorenz-69-attractor">Lorenz 69 attractor</h4>

<p>The following non-linear ODE with a specific configuration of parameters has been the poster-child of chaotic attractors for several decades now.</p>

\[\begin{align}
  \dot{x}(t) = \sigma (y - x)\\
  \dot{y}(t) = x(\rho - z) - y\\
  \dot{z}(t) = xy - \beta z\\ 
  \sigma = 10 \\ 
  \rho = 28 \\ 
  \beta = \frac{8}{3} \\ 
  \begin{bmatrix} x_0 \\ y_0 \\ z_0  \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} \\ 
  t \in [0,100]
\end{align}\]

<p>Viewed clockwise, the figure below shows i) Trajectories of the attractor obtained from a numerical integrator, ii) Time derivative of the trajectories, iii) Chosen bases functions \(\mathcal{B}\) and the corresponding weights and iv) the reconstructed attractor.</p>

<div class="row">
  <div class="column">
    <img style="width:100%" src="/assets/sindy/Traj_Lorenz.png" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:100%" src="/assets/sindy/Vel_Lorenz.png" alt="spline-sur" />
  </div>
</div>

<div class="row">
  <div class="column">
    <img style="width:80%" src="/assets/sindy/Lorenz_Coeffs.png" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:100%" src="/assets/sindy/Remade_Lorenz.png" alt="spline-sur" />
  </div>
</div>

<hr />

<h4 id="lotka-volterra-model">Lotka Volterra model</h4>

<p>Prey predator models are the staple for several ecological, biological and even plasma physical applications. A common model with a single species of prey and predator is shown below.</p>

\[\begin{align}
  \dot{x}(t) = \alpha x(t) + \beta x(t)y(t) \\ 
  \dot{y}(t) = \gamma x(t)y(t) - \delta y(t) \\
  \alpha = 0.7 \\
  \beta = -0.3 \\ 
  \gamma = -0.3 \\ 
  \delta = 0.4 \\ 
  \begin{bmatrix} x_0 \\ y_0  \end{bmatrix} = \begin{bmatrix} 1 \\ 1  \end{bmatrix} \\ 
  t \in [0,100]
\end{align}\]

<p>Akin to the Lorenz attractor, the SINDY algorithm is used to infer the bases weights and the trajetories are then reconstructed.</p>

<div class="row">
  <div class="column">
    <img style="width:100%" src="/assets/sindy/Traj_Lotka.png" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:100%" src="/assets/sindy/Vel_Lotka.png" alt="spline-sur" />
  </div>
</div>

<div class="row">
  <div class="column">
    <img style="width:80%" src="/assets/sindy/LV_Coeffs.png" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:100%" src="/assets/sindy/Remade_Lotka.png" alt="spline-sur" />
  </div>
</div>

<hr />

<h3 id="code">Code</h3>

<p>These results can be reproduced with the following code. <em>(Documentation coming soon.)</em></p>

<script src="https://gist.github.com/dynamic-queries/3edf052b43ec90467dc48f91fe3b552b.js"></script>]]></content><author><name></name></author><category term="education" /><category term="SystemsIdentification" /><summary type="html"><![CDATA[Symbolic regression.]]></summary></entry><entry><title type="html">(Original Research) Full Waveform inversion</title><link href="https://dynamic-queries.github.io/blog/2023/fwi/" rel="alternate" type="text/html" title="(Original Research) Full Waveform inversion" /><published>2023-07-20T08:00:00+00:00</published><updated>2023-07-20T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/fwi</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/fwi/"><![CDATA[<p>This is original work done as a part of a student research project during my masters studies at TUM with <a href="https://fd-research.com/">Felix Dietrich</a>. Part of this work was presented at <a href="https://meetings.siam.org/sess/dsp_talk.cfm?p=125969">SIAM CSE 2023 in Amsterdam</a></p>

<p><em>All ideas, figures, equations, examples and code are a work of my own.</em></p>

<h3 id="introduction">Introduction</h3>

<p>Ultrasonic imaging is a versatile tool for visualizing the subsurface properties of structures, which are not readily available for physical observation. In the last few decades this method has proliferated all walks of life, with applications ranging from medical imaging to non-destructive testing in engineering and the like. There have also been several adapatations such as photo-acoustic imaging which is a crucial tool in imaging microscopic elements of biological matter.</p>

<p>Here I am interested in non-destructive testing for engineering structures. Consider the idealized scenario illustrated below.</p>

<style>
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}
</style>

<p><img style="width:50%" class="center" src="/assets/FWI_edit/outline.svg" alt="spline-sur" /></p>

<p>There is a rectangular domain with a defect in it. One does not know apriori how the defect looks like. But one has access to sensors measuring the amplitude of waves on the surface of the domain. Full waveform inversion concerns reconstructing the shape of the defect from these measured signals.</p>

<h3 id="method">Method</h3>

<p>In an ideal setting, the waves inside the domain \(\Omega\) can be assumed to propagate with velocity \(\forall x \in \Omega \quad c: x \mapsto \mathbb{R}\) and evolving in accordance with the acoustic wave equation.</p>

\[\begin{align}
    \frac{\partial^2 u(x,t)}{\partial t^2} = \frac{\partial^2 }{\partial x^2}\left[c(x)^2 u(x,t)\right] \\ 
    u(x,0) = f(x) \\ 
    \frac{\partial u(x,0)}{\partial t} = g(x) \\ 
    u(y,t) = 0 \:\: \forall y \in \delta \Omega
\end{align}\]

<p>Here, I assume that the shape, \(s(x)\) of the defect is related to the velocity \(c(x)\) through \(s(x) = \phi(c(x))\) for some known invertible function \(\phi\). From this, it can be argued that inferring \(c(x)\) from reference measurement \(u_m(z,t) \:\: \forall z \in \Omega_s\) is, in effect, full-wave form inversion. Or equivalently one seeks the minimizer for the following:</p>

\[\begin{equation}
    \arg \min_{c \in \mathcal{H}} \|u(z,t;c(x)) - u_m(z,t)\|
\end{equation}\]

<h3 id="procedure">Procedure</h3>

<p>I illustrate my approach to this problem with a synthetic example.</p>]]></content><author><name></name></author><category term="projects" /><category term="InverseProblems," /><category term="Surrogatization" /><summary type="html"><![CDATA[Computational Non-destructive testing]]></summary></entry><entry><title type="html">Learning Hamiltonians with GP</title><link href="https://dynamic-queries.github.io/blog/2023/hamiltonian/" rel="alternate" type="text/html" title="Learning Hamiltonians with GP" /><published>2023-07-20T08:00:00+00:00</published><updated>2023-07-20T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/hamiltonian</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/hamiltonian/"><![CDATA[<p>This work is a re-creation of a method for learning the Hamiltonian of a dynamical system from <a href="https://pubs.aip.org/aip/cha/article/29/12/121107/1027304/On-learning-Hamiltonian-systems-from-data">Bertlan and Dietrich</a>.</p>

<p><em>All figures, equations, examples and code are a work of my own. Only the idea has been borrowed from the paper.</em></p>

<h3 id="introduction">Introduction</h3>

<p>Most physical systems lend themselves to description using Hamiltonian mechanics. Briefly put:</p>

<p>For an \(N\) body system living in \(d\) dimensions, the classical mechanical state of the system is described sufficiently using a pair of canonical coordinates \(q,p \in \mathbb{R}^{dN}\). For an evolutionary system, the canonical coordinates depend on time. Evidently, such evolutions are governed by Newton’s equations. However, Netwon’s equation are bulky to compute, analyze for large \(N\). This problem was recognized by Hamilton and subsequently led to what is now referred to the Hamiltonian formulation of classical mechanics.</p>

<p>Hamilton postulated that there exists a function \(H : q \times p \mapsto \mathbb{R}\) – the Hamiltonian. The evolution equations in this framework were given by the following PDE:</p>

\[\begin{align}
    \label{eq:Ham}
    \frac{\partial}{\partial t} q(t) = \frac{\partial H}{\partial p}\\ 
    \frac{\partial}{\partial t} p(t) = -\frac{\partial H}{\partial q}
\end{align}\]

<p>Or more compactly</p>

\[{\partial_t} z := {\partial_t}\begin{bmatrix} q \\ p \end{bmatrix} = \begin{bmatrix} \mathcal{0} &amp;&amp;  \mathcal{I} \\ -\mathcal{I} &amp;&amp; \mathcal{0} \end{bmatrix} \partial_{\begin{bmatrix} q &amp; p \end{bmatrix}^{T}} H(q,p)\]

<p>Given observations \(\left([q(t_1),q(t_2),...,q(t_m)],[p(t_1),p(t_2),...,p(t_m)]\right)\), I am interested in learning \(H\) using a Gaussian process.</p>

<h3 id="method">Method</h3>

<p>Let \(H(z) \sim \mathcal{GP(\mu_z,\kappa(z,z'))}\).</p>

\[\begin{align}
  \mathbb{E}(H(z^*)) = k(z^* ,z)\: k(z ,z')^{-1}\:H(z') \\
  \frac{\partial}{\partial z^*}\mathbb{E}(H(z^*)) = \mathcal{J}^{-1} \frac{\partial z^*}{\partial t} \\
  \left[\frac{\partial}{\partial z^*}k(z^* ,z)\right]\: \left[k(z ,z')\right]^{-1}\:H(z') =\mathcal{J}^{-1} \frac{\partial z^*}{\partial t} 
\end{align}\]

<p>Here, \(z^*\) and \(\frac{\partial z^*}{\partial t}\) are known reference points. \(z'\) are the validation points where the derivative of the Hamiltonian is not known. In order to determine this, one proceeds according to the following algorithm.</p>

<hr />

<p><strong><em>Algorithm</em></strong></p>

<p>Given</p>
<ol>
  <li>Snapshots \(z^*(t_d) = \left[z^*(1), z^*(2) ... z^*(T) \right]\)</li>
  <li>Query points \(z_1, z_2, ...,z_N\)</li>
  <li>Covariance kernel \(\kappa\)</li>
</ol>

<p>Do</p>
<ol>
  <li>Evaluate \(\frac{d z^*}{dt}\) using any higher order finite difference scheme.</li>
  <li>Setup \(b:=\mathcal{J}^{-1} \: \frac{d z^*}{dt}\)</li>
  <li>Compute \(K_{zz'} \in \mathbb{R}^{N \times N}\) where \(K_{zz'}^{i,j} := \kappa(z^i,z^j)\)</li>
  <li>Invert \(K_{zz'}\)</li>
  <li>Compute \(M \in \mathbb{R}^{2dN \times N}\) where \(M^{:,j} := \frac{\partial}{\partial z^*} \kappa(z^*,z)\)</li>
  <li>Evaluate inference matrix \(I := M K_{zz'}^{-1}\)</li>
  <li>Solve \(I H_{z'} = b\) for \(H_{z'}\)</li>
</ol>

<hr />

<h3 id="examples">Examples</h3>

<h5 id="simple-non-linear-pendulum">Simple non-linear pendulum</h5>

<p>A pendulum is a prototypical example of a Hamiltonian dynamical system. As shown below it is a one-dimensional dynamical system, described by the amplitude of oscillations \(\theta\) and its time derivative \(\dot{\theta}\).</p>

<style>
    .column {
  float: left;
  width: 50.00%;
  margin : 0 0 0px 0px;
  padding: 2px;
}

/* Clear floats after image containers */
.row::after {
  content: "";
  clear: both;
  display: table;
}
</style>

<style>
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}
</style>

<p><img style="width:15%" class="center" src="/assets/hamiltonian/pendulum.svg" alt="spline-sur" /></p>

<p>The equations of motion of a pendulum is the classical equation 
\(\begin{align}
    \frac{d^2 \theta}{dt^2} = -\frac{g}{l} sin(\theta)   
\end{align}\)
which when solved for an ensemble of initial conditions, results in its phase space. The phase space and its tangent bundle (its derivative) is shown below.</p>
<div class="row">
  <div class="column">
    <img style="width:100%" src="/assets/hamiltonian/mini_sampled_trajectories.svg" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:100%" src="/assets/hamiltonian/mini_sampled_velocities.svg" alt="spline-sur" />
  </div>
</div>

<p>Note that all the points in the images above satisfies the PDE in (\ref{eq:Ham}). Therefore, one could uniformly draw samples from this space to learn its Hamiltonian. (<em>The figures above show 500 unique samples in red.</em>)</p>]]></content><author><name></name></author><category term="education" /><category term="SystemsIdentification" /><summary type="html"><![CDATA[Inferring invariants of dynamical systems with gaussian processes.]]></summary></entry></feed>