<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://dynamic-queries.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://dynamic-queries.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-07-06T19:56:04+00:00</updated><id>https://dynamic-queries.github.io/feed.xml</id><title type="html">Rahul Manavalan</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">(WIP) Gaussian Quadrature</title><link href="https://dynamic-queries.github.io/blog/2023/gaussian_quad/" rel="alternate" type="text/html" title="(WIP) Gaussian Quadrature" /><published>2023-07-04T08:00:00+00:00</published><updated>2023-07-04T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/gaussian_quad</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/gaussian_quad/"><![CDATA[]]></content><author><name></name></author><category term="review" /><category term="DFT" /><summary type="html"><![CDATA[Quadrature with faster convergence]]></summary></entry><entry><title type="html">(WIP) Frobenius Perron Operator</title><link href="https://dynamic-queries.github.io/blog/2023/frobenius_perron/" rel="alternate" type="text/html" title="(WIP) Frobenius Perron Operator" /><published>2023-06-08T08:00:00+00:00</published><updated>2023-06-08T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/frobenius_perron</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/frobenius_perron/"><![CDATA[]]></content><author><name></name></author><category term="review" /><category term="OperatorML" /><summary type="html"><![CDATA[What you need when Diffusion Models make no sense.]]></summary></entry><entry><title type="html">(WIP) Lie Group Integrators</title><link href="https://dynamic-queries.github.io/blog/2023/lie_group/" rel="alternate" type="text/html" title="(WIP) Lie Group Integrators" /><published>2023-06-07T08:00:00+00:00</published><updated>2023-06-07T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/lie_group</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/lie_group/"><![CDATA[]]></content><author><name></name></author><category term="review" /><category term="DifferentialEquations" /><summary type="html"><![CDATA[Structure preserving integrators for Differential Equations.]]></summary></entry><entry><title type="html">(WIP) Exponential Integrators</title><link href="https://dynamic-queries.github.io/blog/2023/expo_int/" rel="alternate" type="text/html" title="(WIP) Exponential Integrators" /><published>2023-05-26T08:00:00+00:00</published><updated>2023-05-26T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/expo_int</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/expo_int/"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Differential Equations are used as modeling tools in several scientific disciplines. Since the advent of affordable computing technologies, a prevalent theme in these fields is to numerically approximate the solutions to these differential equations using numerical integration.</p>

<p>Consider a first order differential equation, formulated as an initial value problem.</p>

\[\begin{align}
\label{eq:IVP}
\frac{d}{dx}y(x) = f(y(x),x) \\
y(x=0) = y_0 \in \mathbb{R}^n
\end{align}\]

<p>The central theme of numerical integration is to transform the solution of the IVP in (\ref{eq:IVP}) to the solution of the following quadrature problem.</p>

\[\begin{equation}
    \label{eq:quad}
    y(x) = y_0 + \int_{x_0}^{x} dz\:f(z) = y_0 + \frac{1}{x-x_0} \int_{0}^{1} dq\:f(q) 
\end{equation}\]

<p>Admittedly, there are several techniques to approximate the integral in (\ref{eq:quad}). Here we are interested in a class of integrators that are suitable for stiff differential equations.</p>

<h2 id="stiff-differential-equations">Stiff Differential Equations</h2>
<p>Stiffness of differential equations is an ambiguous term. Hairer and Wanner famouosly write about stiff differential equations, <em>as equations where evaluation of solutions using explicit integration methods is too inefficient</em>. There have been other attempts to formalize this “definition” using the eigenfunctions of the differential operator. Intuitively, stiff differential equations are those with \(f(y(x),x)\) evolving in disparate scales along its different bases directions.</p>

<h2 id="exponential-integrators">Exponential Integrators</h2>
<p>The question of resolving the disparate timescales in \(f(y(x),x)\) is addressed in exponential integrators as follows.</p>

\[\begin{align}
    \label{eq:exp-int}
    \frac{d}{dx}y(x) = Ay(x) + g(y(x),x) \textrm{  [Linearization] }
\end{align}\]

<p>The solution to the homogenous problem in (\ref{eq:exp-int}), is</p>

\[\begin{align}
    \label{eq:exact-sol}
    y(x) = \exp^{xA} y(0)
\end{align}\]

\[\begin{equation}
    \label{eq:exp}
    \exp^{zA} = \mathcal{I} + \frac{zA}{1!} + \frac{(zA)^2}{2!} + \frac{(zA)^3}{3!} + ...
\end{equation}\]

<p>For a generic heterogenity, \(g(x)\), the solution is expressed in terms of its propagator.</p>

\[\begin{equation}
    \label{eq:hetero}
    y(x) = \exp^{xA} y(0) + \int dz \: \exp^{(x-z)A} g(y(z),z)
\end{equation}\]]]></content><author><name></name></author><category term="review" /><category term="DifferentialEquations" /><summary type="html"><![CDATA[ODE Integrators for Stiff Differential Equations.]]></summary></entry><entry><title type="html">(WIP) Numerical Methods for Density Functional Theory</title><link href="https://dynamic-queries.github.io/blog/2023/dft/" rel="alternate" type="text/html" title="(WIP) Numerical Methods for Density Functional Theory" /><published>2023-05-11T08:00:00+00:00</published><updated>2023-05-11T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/dft</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/dft/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>It has been known for years that the physical laws underlying any arbitrary physical systems can be accurately described using quantum mechanics.
While this viewpoint is philosophically elegant, it is not really useful as quantum mechanical descriptions of many-particle interactions, is formulated in a Hilbert’s space which scales exponentially with the number of particles, posing a computational bottleneck, which Bellman called the <strong>Curse of Dimensionality</strong>.</p>

<p>One can often get away with this, by modelling systems with mean field, classical particle based and even continuum based approximations as one often does in quantum field theory, kinetic theory of fluids and fluid mechanics respectively. <a href="https://en.wikipedia.org/wiki/Spectroscopy">Molecular Spectroscopy</a> is one discipline where errors induced by such approximations is often dire, requiring explicit quantum mechanical descriptions despite the aforementioned bottleneck due to Bellman.</p>

<p>Luckily there are several approximations that have been proposed that aim at circumventing this problem. One such method is Density Functional Theory. In what follows, I present a self-consistent introduction to Density Functional Theory (DFT) from the perspective of numerical analysis and an efficient algorithmic implementation.</p>

<h2 id="a-many-body-problem">A Many Body Problem</h2>
<p>For the rest of the article, I will refer to a many body quantum system with \(n\) electrons and \(m\) nuclear centers. It is known that the state of this system can be completely described with the help of its wavefunction, \(|\Psi\rangle\). If, the quantum system has \(p\) spin states, then the wavefunction, \(|\Psi\rangle : \mathbb{R}^{p^{m+n}} \times \mathbb{R}^{+} \mapsto \mathbb{C}\). Its evolution, is defined axiomatically by Schrodinger’s equation:</p>

\[\begin{equation}
    \label{eq:MBP}
    i\bar{h} \frac{\partial |\Psi\rangle}{\partial t} = \left( -\frac{\bar{h}^2}{2M} \nabla^2 + V \right) |\Psi\rangle
\end{equation}\]

<p>The potential operator \(V\) in (\ref{eq:MBP}) is usually dominated by the electrostatic interactions among the different charge centers in the system. If \(Z_1, Z_2\) are the atomic numbers of two nuclear centers, located at \(r_1, r_2\), then the electrostatic potential between the two is</p>

\[\begin{equation}
    \label{eq:electrostatic}
    V(r_1,r_2) = \frac{1}{4 \pi \epsilon} \frac{Z_1 Z_2 e^2}{\|r_1 - r_2\|} + V_{ext}
\end{equation}\]

<p>It is common to expand out (\ref{eq:MBP}-\ref{eq:electrostatic}) in its long form as follows.</p>

\[\begin{equation}
    \label{eq:Ham}
    \begin{aligned}
        i\bar{h} \frac{\partial |\Psi\rangle}{\partial t} = \left( -\frac{\bar{h}^2}{2} \sum_{i=1}^{m} \frac{1}{M_i}\nabla_{r_i}^2 -\frac{\bar{h}^2}{2} \sum_{j=1}^{n} \frac{1}{Me_{i}}\nabla_{r_j}^2\\ 
         + \frac{1}{4 \pi \epsilon} \sum_{i=i}^{n} \sum_{j=1}^{m} \frac{Z_i e^2}{\|r_i - r_j\|} + \frac{1}{4 \pi \epsilon} \sum_{i=i}^{n} \sum_{j=1}^{n} \frac{e^2}{\|r_i - r_j\|} + \frac{1}{4 \pi \epsilon} \sum_{i=i}^{m} \sum_{j=1}^{m} \frac{Z_i Z_j e^2}{\|r_i - r_j\|} + V_{ext}\right) |\Psi\rangle
    \end{aligned}
\end{equation}\]

<p>The long form operator is known as the many electronic Hamiltonian. By itself, equation (\ref{eq:Ham}) is a parabolic partial differential equation with no analytical solutions, except for very simplified cases.</p>

<p>The alternative of simulating the system numerically is also not computationally tractable. Say, \(p=2, m=6, n=60\). This corresponds to \(\Psi\) with \(2^{66} \approx 7.37 \times 10^{19}\) input parameters. If one were to parameterize each of these input directions with only \(10\) indicator functions then the state vector \(\tilde{\Psi}\) that uniquely represents a quantum state has \(10 \times 2^{66}\) components. This poses a burden that would be too heavy to bear for systems of larger sizes.</p>

<h2 id="born-oppenheimer-approximation">Born Oppenheimer approximation</h2>

<p>Making the observation that nuclear centers are substantially more massive than the electrons, Born and Oppenheimer postulated that the electrons and nuclear centers evolve in two different timescales. Effectively, the contributions of the nuclei to the many electronic Hamiltonian can be dropped and the nuclei can in-turn be simulated using Newton’s laws govered by a potential surface \(E\) obtained from solving the stationary Schrodinger’s equation.</p>

\[\begin{equation}
    \label{eq:SSE}
        H_e |\Psi\rangle := \left(-\frac{\bar{h}^2}{2} \sum_{j=1}^{n} \frac{1}{Me_{i}}\nabla_{r_j}^2\\ 
         + \frac{1}{4 \pi \epsilon} \sum_{i=i}^{n} \sum_{j=1}^{m} \frac{Z_i e^2}{\|r_i - r_j\|} + \frac{1}{4 \pi \epsilon} \sum_{i=i}^{n} \sum_{j=1}^{n} \frac{e^2}{\|r_i - r_j\|} + V_{ext}\right) |\Psi\rangle = E |\Psi\rangle
\end{equation}\]

<p>While, the BO approximation certainly reduces the complexity of solving (\ref{eq:MBP}), the wavefunction is still sufficiently massive to discourage its  computation, even on the largest computational facilities on earth. This is the curse of dimensionality that I alluded to, in the introduction. In what follows, I will elaborate on how DFT resolves this problem.</p>

<h2 id="density-functional-theory">Density Functional Theory</h2>
<p>\(|\Psi\rangle(z_1,z_2,....)\) has an exponentially growing input parameter space. So, all computations that involve \(|\Psi\rangle\) are prohibitively expensive. DFT addresses this problem, by restating (\ref{eq:SSE}) using an observable of \(|\Psi\rangle\) that takes in fewer parameters – the electron density of the system, \(\rho\), which is defined as:</p>

\[\begin{equation}
    \label{eq:density}
    \rho(r) = n \sum_{s=\{0,1\}} \int \: dz_2 dz_3 ... dz_n \: \Psi^{*}((r,s),z_2,...,z_n) \Psi((r,s),z_2,...,z_n)
\end{equation}\]

<p>It would seem, that computing the density, in-turn requires knowledge of the wavefunction in the first place, which we have established as difficult to assemble on a computer. So, is this whole notion moot? Yes and No. While it is not computationally tractable to compute \(\rho\) directly, one can reformulate (\ref{eq:SSE}) in a variational setting and optimize for \(\rho\) indirectly.</p>

<p>Consider the Ritz minimization iteration in the context of (\ref{eq:SSE}).</p>

<p>\(\begin{equation}
    E_{min} = \arg \min_{|\Psi\rangle \in \Theta} \frac{\langle\Psi|H_e|\Psi\rangle}{\langle\Psi|\Psi\rangle}
\end{equation}\)
where \(\Theta\) is the hypothesis space of antisymmetric functions \(|\Psi\rangle\) with \(\langle\Psi|\Psi\rangle = 1\).</p>]]></content><author><name></name></author><category term="review" /><category term="DFT" /><summary type="html"><![CDATA[A Primer on Electronic Structure Calculations.]]></summary></entry><entry><title type="html">(WIP) Generative modeling using Diffusion map particle system</title><link href="https://dynamic-queries.github.io/blog/2023/gen_dmap/" rel="alternate" type="text/html" title="(WIP) Generative modeling using Diffusion map particle system" /><published>2023-05-06T08:00:00+00:00</published><updated>2023-05-06T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/gen_dmap</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/gen_dmap/"><![CDATA[<h3 id="introduction">Introduction</h3>

<p>This review aims at providing a self-contained, pedantic overview of deep generative modelling using score based diffusion, culminating with discussions on <a href="https://arxiv.org/abs/2304.00200">the recent paper</a> from Youssef Marzouk.</p>

<p>Generative models are manifestations of this widely held philosophical belief:</p>
<div class="alert alert-block alert-success">
<b>Philosophical Belief 1</b> All natural phenomena are generative. That is to say, there is an underlying generative process for every physical occurance.
</div>

<p>Clearly, there are infinitely many possibilities that may ensue from such a generative process. Assuming that we can enumerate all of these outcomes, say with set \(\Omega\), then a generative process \(\mathcal{G}\) underlying a system \(\mathcal{S}\) can be effectively described using probability theory. Therefore, the problem of generative modelling is in-effect the problem of probability density (distribution) estimation.</p>

<p>In practice, there are several restrictions to effectively estimating probability density, \(p_{\mathcal{G}}(z) \: \forall z \in \Omega\), from a finite set of observations \(\Omega_f \subset \Omega\). Regardless, any method \(m\) that claims to do so must satisfy these conditions.</p>
<ol>
  <li>For a suitably chosen functional norm, \(\lim_{\Omega_f \mapsto \Omega} \|p_{\mathcal{G}}^{m}(z) - p_{\mathcal{G}}(z)\|_f \mapsto 0\)</li>
  <li>It must be computationally effcient to sample from \(p_{\mathcal{G}}^{m}(z)\).</li>
  <li>It must be computationally effcient to evaluate \(p_{\mathcal{G}}^{m}(z)\).</li>
</ol>

<p>Traditional functional approximation methods can satisfy (1) with an arbitrary level of accuracy, yet fail with respect to (2) and (3) due to the curse of dimensionality. Deep learning models on the other hand, are adept at circumventing this problem. Popular deep-generative models (in no particular order) include</p>

<div class="alert alert-block alert-success">
   1. Adversarial Generative Models <br />
   2. Variational Autoencoders <br />
   3. Normalizing Flows <br />
   4. Score based Diffusion Models <br />
   5. Autoregressive Models <br />
   6. Energy based Models <br />
</div>
<p>to name a few.</p>

<p>In what follows, a tutorial styled introduction to score based diffusion algorithms is presented and are compared with the method using diffusion map particle system.</p>

<h3 id="score-based-diffusion">Score based diffusion</h3>

<p>Diffusion models are based on certain assumptions and results, which we will assume are axiomatic.</p>

<ol>
  <li>Let \(p_f\) be the probability distribution defined over a certain function space \(\mathcal{F}\) such that \(\forall f \in \mathcal{F}, f:x \mapsto y\) where \(x,y \in \mathbb{R}^d\).</li>
  <li>Let there exist a time dependent stochastic differential operator \(\mathcal{L}_t\) which transforms \(f(x) \sim p_f\) such that \(\lim_{t \mapsto \infty} \mathcal{L}_t f(x) = g(x)\) where \(g(x) \sim \mathcal{GP}(\bar{x},k(x,x'))\).</li>
  <li>For every such forward operator, there exists an equivalent reverse time stochastic differential operator \(\tilde{\mathcal{L}_t}\) such that \(\lim_{t \mapsto \infty} \tilde{\mathcal{L}_t} g(x) = f(x)\).</li>
</ol>

<p>2 and 3, taken together results in differentiable and invertible map from a distribution over a trivial function space (noisy functions in this case) to a distribution over functions with certain structure. This forms the crux of generative modeling using score based diffusion. The key idea is to sample or evaluate the density of functions on the trivial distribution and transform the samples back and forth by solving stochastic differential equations. This further leads to the following questions.</p>

<ol>
  <li>For a given \(\mathcal{F}\), what is the most appropriate choice of \(\mathcal{L}\) in terms of computational tractablity? That is,
    <ol>
      <li>Which stochastic differential equation, reaches its steady state relatively quickly?</li>
      <li>Which numerical integration scheme preserves the structure of any \(f \sim \mathcal{F}\)?</li>
    </ol>
  </li>
  <li>What is the dual operator \(\tilde{\mathcal{L}}\) for a given \(\mathcal{L}\)?</li>
</ol>

<p>For better or worse, there is no clear resolution on the “best SDE” and the “best numerical integrator” for a given function class. Most of today’s state of the art, in this regard, is based on heuristics.</p>

<h4 id="computational-model">Computational model</h4>
<p>Let \(p_0(x)\) be the target distribution. Given sample \(x_i^0 \in \mathbb{R}^d \sim p_0(x)\), we proceed as follows.</p>

<ol>
  <li>Numerically solve \(dx^t = f(x^t,t)\:dt + g(t)\:dW(t)\) with initial condition \(x_i^0\) for \([0,T]\).
    <ol>
      <li>Here, \(f(x,t) \textrm{ and } g(t)\) are known as the drift and the diffusivity terms respectively.</li>
      <li>\(W(t)\) is a Wiener process.</li>
    </ol>
  </li>
  <li>Parmeterize a neural network \(\mathcal{S_{\Theta}}:\mathbb{R}^d \mapsto \mathbb{R}^d\).</li>
  <li>Numerically solve \(dx^t = [f(x^t,t) - g(t)^2 \mathcal{S_{\Theta}}]\:dt + g(t)\:dW(t)\) for \([T,0]\).</li>
</ol>

<p>Use this computational routine to solve</p>

\[\begin{equation}
   \arg \min_{\Theta} \mathbb{E}_{x \sim p(x)}[\|m^{0 \mapsto T \mapsto 0}(x) - x\|]
\end{equation}\]

<p>Common choices for \(f,g\) in literature include:</p>

<h5 id="sldm">SLDM</h5>
<p>\(\begin{equation}
   f(x,t) = -\frac{\beta(t)x}{2}
\end{equation}\)</p>

\[\begin{equation}
   g(t) = \sqrt{\beta(t)}
\end{equation}\]

<h5 id="ddpm">DDPM</h5>

\[\begin{equation}
   f(x,t) = 0
\end{equation}\]

\[\begin{equation}
   g(t) = \sqrt{\frac{d \sigma(t)}{dt}}
\end{equation}\]

<p>for some suitably chosen functions \(\sigma, \beta\).</p>]]></content><author><name></name></author><category term="review" /><category term="GenMod" /><summary type="html"><![CDATA[Marrying two unlikely algorithms for physical modeling.]]></summary></entry></feed>