<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://dynamic-queries.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://dynamic-queries.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-07-23T00:40:42+00:00</updated><id>https://dynamic-queries.github.io/feed.xml</id><title type="html">Rahul Manavalan</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">(Original work) Random feature potentials</title><link href="https://dynamic-queries.github.io/blog/2023/random_models/" rel="alternate" type="text/html" title="(Original work) Random feature potentials" /><published>2023-07-22T08:00:00+00:00</published><updated>2023-07-22T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/random_models</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/random_models/"><![CDATA[<p>This is original work done as a part of my masters studies at TUM with <a href="http://christian.mendl.net/">Chrisitian Mendl</a> and <a href="https://fd-research.com/">Felix Dietrich</a>.</p>

<p><em>All ideas, figures, equations, examples and code, unless cited, are a work of my own.</em></p>]]></content><author><name></name></author><category term="projects" /><category term="Surrogatization" /><summary type="html"><![CDATA[Inferring potential energy surfaces with random feature models.]]></summary></entry><entry><title type="html">Learning effective SDEs with neural networks</title><link href="https://dynamic-queries.github.io/blog/2023/sde/" rel="alternate" type="text/html" title="Learning effective SDEs with neural networks" /><published>2023-07-22T00:00:00+00:00</published><updated>2023-07-22T00:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/sde</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/sde/"><![CDATA[<p>This work is a re-creation of a method for learning effective stochastic differential equations modeling a dynamical system from <a href="https://arxiv.org/abs/2106.09004">Dietrich et al</a>. It also introduces numerical methods for integrating stochastic differential equations.</p>]]></content><author><name></name></author><category term="projects" /><category term="SystemsIdentification" /><category term="SDEs" /><summary type="html"><![CDATA[Coarse grained modeling]]></summary></entry><entry><title type="html">(Original work) Bayesian Waveform Inversion</title><link href="https://dynamic-queries.github.io/blog/2023/bayesian_inversion/" rel="alternate" type="text/html" title="(Original work) Bayesian Waveform Inversion" /><published>2023-07-21T08:00:00+00:00</published><updated>2023-07-21T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/bayesian_inversion</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/bayesian_inversion/"><![CDATA[<p>This is original work done as a part of a student research project during my masters studies at TUM with <a href="https://fd-research.com/">Felix Dietrich</a>.</p>

<p><em>All ideas, figures, equations, examples and code, unless cited are a work of my own.</em></p>

<h3 id="introduction">Introduction</h3>

<p><strong>Waveform inversion</strong> is principally parameter estimation with three insights.</p>

<ol>
  <li>The parameter is a field over some domain \(\Omega\).</li>
  <li>Only partial observations of the quantity of interest are available.</li>
  <li>Observables are obtained from evaluating a family of wave propagation models.</li>
</ol>

<p>Consider a simple example of an acoustic wave equation.</p>

\[\begin{align}
    x \in \Omega \subset \mathbb{R} \\ 
    t \in \mathbb{R}^+ \\ 
    \frac{\partial}{\partial t}  u(x,t) = \frac{\partial^2}{\partial x^2} \left[\frac{1}{c(x)^2} u(x,t)\right] \\ 
    u(x,0) = f(x) \\ 
    \frac{\partial }{\partial t}u(x,0) = g(x) \\
    u(y,t) = 0 \quad \forall y \in \delta \Omega
\end{align}\]

<p>Here \(f,g\) are suitably chosen initial conditions that would sustain elastic progation of a wave in \(\Omega\). In the context of waveform inversion, \(c(x)\) is the parameter and \(u(z,t) \: \forall z \in \hat{\Omega} \subset \Omega\) is the observable. For the sake of this article, consider the following example for \(c,u\).</p>

<style>
    .column {
  float: left;
  width: 50.00%;
  margin : 0 0 0px 0px;
  padding: 2px;
}

/* Clear floats after image containers */
.row::after {
  content: "";
  clear: both;
  display: table;
}
</style>

<div class="row">
  <div class="column">
    <img style="width:100%" src="/assets/bayesian/parameter.svg" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:100%" src="/assets/bayesian/observable.svg" alt="spline-sur" />
  </div>
</div>

<p>The goal is to reconstruct \(c(x)\) (<em>to the left</em>) from \(u(z,t)\) (<em>to the right</em>) measured at \(z=\{z_0, z_i\}\) where \(z_i \in \Omega\). The optimal \(c^{*}(x)\) is the minimizer to</p>

\[\begin{align}
  \label{eq:opt}
  \arg \min_{\hat{c} \in \mathcal{H}} \sum_{i}\| \hat{u}(z_i,t; \hat{c}(x)) - u(z_i,t)\|
\end{align}\]

<p>where \(\hat{u}(z_0,t)\) is evaluated by solving the acoustic wave equation using a robust finite difference method <em>(see code)</em>.</p>

<h3 id="parameter-estimation">Parameter estimation</h3>
<p>Estimating \(c(x)\) is non-trivial. Firstly, one has to choose an appropriate bases to parameterize this function. This of course varies on a case by case basis. For the example above, radial bases functions are good approximants. Therefore,</p>

\[\begin{align}
  c(x) = \sum_{i=1}^{N} a_i \phi_i (x) \\ 
  \phi_i(x) := \phi(x;x_i,\mu) = \exp\left[{-\left(\frac{x-x_0}{\mu}\right)^2}\right] \\ 
  N = 5
\end{align}\]

<p>Therefore the parameterization space is now \(\mathcal{A} \ni a\).</p>

<p>\(N\) is, by design, chosen sufficiently small to faciliate comparison with Bayesian optimization.</p>

<h4 id="frequentist-estimation">Frequentist estimation</h4>
<p>Problem (\ref{eq:opt}) is ill-posed in the sense of Hadamard. As a result, it is initially solved with tradional gradient based optimization algorithms with suitable regularization. The algorithms used in this pursuit, are briefy outlined and the “best possible” reconstructions are presented.</p>

<p>The reconstructions here, must be taken with a grain of salt, for it is entirely possible to come up with an excellent initial guess that could converge to the actual \(c(x)\) accurately.</p>

<p>Furthermore, it is observed that the reconstructions are sensitive to the choice of the type of regularization and the magnitude of the regularization constant. Common regularizations to an \(l_2\) loss includes weighted norms of inputs, finite difference of \(l_2\) loss with respect to time, fourier coefficients of \(l_2\) loss and so on.</p>

<p>In the presented cases, \(l_2\) Tikhonov regularization is used. The values of the regularization constants and the number of iterations used for each of these algorithms is indicated below.</p>

<h5 id="adam">ADAM</h5>

<hr />
<p><em>Algorithm</em></p>

<hr />

<p>Regularization constant \(\lambda = 2.01 \pm 10^{-4}\)</p>

<p><img style="width:60%" src="/assets/bayesian/recon_parameter_adam.svg" alt="spline-sur" /></p>

<h5 id="bfgs">BFGS</h5>

<hr />
<p><em>Algorithm</em></p>

<hr />

<p>Regularization constant \(\lambda = 0.399 \pm 10^{-3}\)</p>

<p><img style="width:60%" src="/assets/bayesian/recon_parameter_bfgs.svg" alt="spline-sur" /></p>

<h5 id="issues">Issues</h5>

<p>There are two difficulties that one encounters with the frequentist estimation while treating an ill-conditioned problem such as this one.</p>

<ol>
  <li>Good initial guesses are vital for fast convergence.</li>
  <li>Estimation is very sensitive to the value of \(\lambda\).</li>
</ol>

<p>Elaborating on (2), consider the following reconstructions with BFGS optimization for \(\lambda = 0.397,\:0.399\:,0.40\:,0.402\)  respectively.</p>

<div class="row">
  <div class="column">
    <img style="width:80%" src="/assets/bayesian/bfgs_0.397.svg" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:80%" src="/assets/bayesian/bfgs_0.399.svg" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:80%" src="/assets/bayesian/bfgs_0.4.svg" alt="spline-sur" />
  </div>
  <div class="column">
    <img style="width:80%" src="/assets/bayesian/bfgs_0.402.svg" alt="spline-sur" />
  </div>
</div>

<p>Such sensitivity makes any practical grid search (which is performed at a much higher resolution) useless, questioning the practicality of such optimization routines for practical applications.</p>

<h4 id="bayesian-estimation">Bayesian estimation</h4>
<p>Plagued by the ill-posedness of frequentist estimation, it seems appropriate to turn to Bayesian estimation. Granted, one does need a good prior for Bayesian methods to make computational sense, however Bayesian inverse problems are well posed (As highlighted in <a href="https://www.cambridge.org/core/journals/acta-numerica/article/abs/inverse-problems-a-bayesian-perspective/587A3A0D480A1A7C2B1B284BCEDF7E23">Stuart</a>). In what follows, two classes of Bayesian state estimation methods – MCMC and HMC are used to estimate \(c(x)\). Relative merits and pitfalls, in contrast with frequentist methods are discussed.</p>

<h5 id="markov-chain-monte-carlo">Markov Chain Monte carlo</h5>

<h5 id="hamiltonian-monte-carlo">Hamiltonian Monte carlo</h5>

<h3 id="code">Code</h3>

<p>These results can be reproduced with the following code. <em>(Documentation coming soon.)</em></p>

<script src="https://gist.github.com/dynamic-queries/3473ab7b87744cd1ad07cddd74eabd03.js"></script>]]></content><author><name></name></author><category term="projects" /><category term="InverseProblems" /><summary type="html"><![CDATA[Full waveform inversion with MCMC, HMC, NUTS]]></summary></entry><entry><title type="html">ODE Filters</title><link href="https://dynamic-queries.github.io/blog/2023/ode_filters/" rel="alternate" type="text/html" title="ODE Filters" /><published>2023-07-21T08:00:00+00:00</published><updated>2023-07-21T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/ode_filters</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/ode_filters/"><![CDATA[]]></content><author><name></name></author><category term="projects" /><category term="ProbNumerics" /><summary type="html"><![CDATA[Integrating ODEs with Gaussian filters]]></summary></entry><entry><title type="html">SINDY - Sparse Identification of non-linear dynamical systems.</title><link href="https://dynamic-queries.github.io/blog/2023/sindy/" rel="alternate" type="text/html" title="SINDY - Sparse Identification of non-linear dynamical systems." /><published>2023-07-20T08:00:00+00:00</published><updated>2023-07-20T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/sindy</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/sindy/"><![CDATA[<p>This work is a re-creation of a symbolic regression method from <a href="https://www.pnas.org/doi/10.1073/pnas.1517384113">Brunton and Kutz</a>. The article is meant to be a quick summary of this method.</p>

<p><em>All figures, equations, examples and code are a work of my own. Only the idea has been borrowed from the paper.</em></p>

<h3 id="introduction">Introduction</h3>

<p>Data driven methods are the modern day equivalent to Poincare’s work in the 19th century. A characteristic feature of these methods is determining the equations of motion underlying a given dynamical system, exclusively from time dependent observables, measured from that system.</p>

<hr />
<p>Consider this generic form of a dynamical system.</p>

\[\begin{align}
    \frac{dx(t)}{dt} = f(x(t)) \\ 
    x(0) = x_0 \in \mathbb{R}^d
\end{align}\]

<p>Given \(X := [x(t_1),x(t_2),...,x(t_N)]\), we would like to infer \(f\) either symbolically or numerically.</p>

<hr />

<h3 id="sindy">SINDY</h3>

<p>SINDY is a symbolic regression algorithm that identifies \(f\), provided one has access to the most apposite bases set, say \(\mathcal{B}\), that spans the function space \(\mathcal{F} \ni f\). It’s usually the case that for most systems, knowledge of \(\mathcal{B}\) is sparse at best; raising serious questions about the practicality of this method for data arising from real world applications. Nonetheless it is a nice method where one has expert knowledge about the system. The algorithm has four main steps.</p>

<hr />

<p><strong><em>Algorithm</em></strong></p>

<ol>
  <li>
    <p>Evaluate \(\dot{X} := [\frac{dx(t_1)}{dt},\frac{dx(t_2)}{dt},...,\frac{dx(t_N)}{dt}] \in \mathbb{R}^{d \times N}\) using a sufficiently robust finite difference scheme.</p>
  </li>
  <li>
    <p>Choose a bases set \(\mathcal{B} := \{b_1(x), b_2(x), ..., b_k(x)\}\) of functions.</p>
  </li>
  <li>
    <p>Evalute \(\Theta(X) := \begin{bmatrix} b_1(x(t_1)) &amp;&amp; ... &amp;&amp; b_1(x(t_N)) \\ \vdots &amp;&amp;  &amp;&amp; \vdots \\  b_k(x(t_1)) &amp;&amp; ... &amp;&amp; b_k(x(t_1)) \\ \end{bmatrix} \in \mathbb{R}^{k \times d \times N}\)</p>
  </li>
  <li>
    <p>If \(\dot{X} = K \: \Theta(X)\) where \(K \in \mathbb{R}^{k}\). Solve for \(K\) using linear least squares and a sparsity inducing regularizer.</p>
  </li>
</ol>

<hr />

<h3 id="examples">Examples</h3>

<p>Here the algorithm is applied to commonly recurring examples in literature – Lorenz 69 attractor and a Lotka-Volterra model.</p>

<style>
    .column {
  float: left;
  width: 50.00%;
  margin : 0 0 0px 0px;
  padding: 2px;
}

/* Clear floats after image containers */
.row::after {
  content: "";
  clear: both;
  display: table;
}
</style>

<h4 id="lorenz-69-attractor">Lorenz 69 attractor</h4>

<p>The following non-linear ODE with a specific configuration of parameters has been the poster-child of chaotic attractors for several decades now.</p>

\[\begin{align}
  \dot{x}(t) = \sigma (y - x)\\
  \dot{y}(t) = x(\rho - z) - y\\
  \dot{z}(t) = xy - \beta z\\ 
  \sigma = 10 \\ 
  \rho = 28 \\ 
  \beta = \frac{8}{3} \\ 
  \begin{bmatrix} x_0 \\ y_0 \\ z_0  \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} \\ 
  t \in [0,100]
\end{align}\]

<p>Viewed clockwise, the figure below shows i) Trajectories of the attractor obtained from a numerical integrator, ii) Time derivative of the trajectories, iii) Chosen bases functions \(\mathcal{B}\) and the corresponding weights and iv) the reconstructed attractor.</p>

<div class="row">
  <div class="column">
    <img style="width:100%" src="/assets/sindy/Traj_Lorenz.png" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:100%" src="/assets/sindy/Vel_Lorenz.png" alt="spline-sur" />
  </div>
</div>

<div class="row">
  <div class="column">
    <img style="width:80%" src="/assets/sindy/Lorenz_Coeffs.png" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:100%" src="/assets/sindy/Remade_Lorenz.png" alt="spline-sur" />
  </div>
</div>

<hr />

<h4 id="lotka-volterra-model">Lotka Volterra model</h4>

<p>Prey predator models are the staple for several ecological, biological and even plasma physical applications. A common model with a single species of prey and predator is shown below.</p>

\[\begin{align}
  \dot{x}(t) = \alpha x(t) + \beta x(t)y(t) \\ 
  \dot{y}(t) = \gamma x(t)y(t) - \delta y(t) \\
  \alpha = 0.7 \\
  \beta = -0.3 \\ 
  \gamma = -0.3 \\ 
  \delta = 0.4 \\ 
  \begin{bmatrix} x_0 \\ y_0  \end{bmatrix} = \begin{bmatrix} 1 \\ 1  \end{bmatrix} \\ 
  t \in [0,100]
\end{align}\]

<p>Akin to the Lorenz attractor, the SINDY algorithm is used to infer the bases weights and the trajetories are then reconstructed.</p>

<div class="row">
  <div class="column">
    <img style="width:100%" src="/assets/sindy/Traj_Lotka.png" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:100%" src="/assets/sindy/Vel_Lotka.png" alt="spline-sur" />
  </div>
</div>

<div class="row">
  <div class="column">
    <img style="width:80%" src="/assets/sindy/LV_Coeffs.png" alt="spline-sim" />
  </div>
  <div class="column">
    <img style="width:100%" src="/assets/sindy/Remade_Lotka.png" alt="spline-sur" />
  </div>
</div>

<hr />

<h3 id="code">Code</h3>

<p>These results can be reproduced with the following code. <em>(Documentation coming soon.)</em></p>

<script src="https://gist.github.com/dynamic-queries/3edf052b43ec90467dc48f91fe3b552b.js"></script>]]></content><author><name></name></author><category term="projects" /><category term="SystemsIdentification" /><summary type="html"><![CDATA[Symbolic regression.]]></summary></entry><entry><title type="html">(Original work) Full Waveform inversion</title><link href="https://dynamic-queries.github.io/blog/2023/fwi/" rel="alternate" type="text/html" title="(Original work) Full Waveform inversion" /><published>2023-07-20T08:00:00+00:00</published><updated>2023-07-20T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/fwi</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/fwi/"><![CDATA[<p>This is original work done as a part of a student research project during my masters studies at TUM with <a href="https://fd-research.com/">Felix Dietrich</a>. Part of this work was presented at <a href="https://meetings.siam.org/sess/dsp_talk.cfm?p=125969">SIAM CSE 2023 in Amsterdam</a></p>

<p><em>All ideas, figures, equations, examples and code are a work of my own.</em></p>

<h3 id="introduction">Introduction</h3>

<p>Ultrasonic imaging is a versatile tool for visualizing the subsurface properties of structures, which are not readily available for physical observation. In the last few decades this method has proliferated all walks of life, with applications ranging from medical imaging to non-destructive testing in engineering and the like. There have also been several adapatations such as photo-acoustic imaging which is a crucial tool in imaging microscopic elements of biological matter.</p>

<p>Here I am interested in non-destructive testing for engineering structures. Consider the idealized scenario illustrated below.</p>

<style>
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}
</style>

<p><img style="width:70%" class="center" src="/assets/FWI_edit/outline.svg" alt="spline-sur" /></p>

<p>There is a rectangular domain with a defect in it. One does not know apriori how the defect looks like. But one has access to sensors measuring the amplitude of waves on the surface of the domain. Full waveform inversion concerns reconstructing the shape of the defect from these measured signals.</p>

<h3 id="method">Method</h3>

<p>In an ideal setting, the waves inside the domain \(\Omega\) can be assumed to propagate with velocity \(\forall x \in \Omega \quad c: x \mapsto \mathbb{R}\) and evolving in accordance with the acoustic wave equation.</p>

\[\begin{align}
    \frac{\partial^2 u(x,t)}{\partial t^2} = \frac{\partial^2 }{\partial x^2}\left[\frac{1}{c(x)^2}u(x,t)\right] \\ 
    u(x,0) = f(x) \\ 
    \frac{\partial u(x,0)}{\partial t} = g(x) \\ 
    u(y,t) = 0 \:\: \forall y \in \delta \Omega
\end{align}\]

<p>Here, I assume that the shape, \(s(x)\) of the defect is related to the velocity \(c(x)\) through \(s(x) = \phi(c(x))\) for some known invertible function \(\phi\). From this, it can be argued that inferring \(c(x)\) from reference measurement \(u_m(z,t) \:\: \forall z \in \Omega_s\) is, in effect, full-wave form inversion. Or equivalently one seeks the minimizer for the following:</p>

\[\begin{equation}
    \arg \min_{c \in \mathcal{H}} \|u(z,t;c(x)) - u_m(z,t)\|
\end{equation}\]

<h3 id="procedure">Procedure</h3>

<p>I illustrate my approach to this problem with a synthetic example.</p>]]></content><author><name></name></author><category term="projects" /><category term="InverseProblems," /><category term="Surrogatization" /><summary type="html"><![CDATA[Computational Non-destructive testing]]></summary></entry><entry><title type="html">Learning Hamiltonians with GP</title><link href="https://dynamic-queries.github.io/blog/2023/hamiltonian/" rel="alternate" type="text/html" title="Learning Hamiltonians with GP" /><published>2023-07-20T08:00:00+00:00</published><updated>2023-07-20T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/hamiltonian</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/hamiltonian/"><![CDATA[<p>This work is a re-creation of a method for learning the Hamiltonian of a dynamical system from <a href="https://pubs.aip.org/aip/cha/article/29/12/121107/1027304/On-learning-Hamiltonian-systems-from-data">Bertlan and Dietrich</a>.</p>

<p><em>All figures, equations, examples and code are a work of my own. Only the idea has been borrowed from the paper.</em></p>

<h3 id="introduction">Introduction</h3>

<p>Most physical systems lend themselves to description using Hamiltonian mechanics. Briefly put:</p>

<p>For an \(N\) body system living in \(d\) dimensions, the classical mechanical state of the system is described sufficiently using a pair of canonical coordinates \(q,p \in \mathbb{R}^{dN}\). For an evolutionary system, the canonical coordinates depend on time. Evidently, such evolutions are governed by Newton’s equations. However, Netwon’s equation are bulky to compute, analyze for large \(N\). This problem was recognized by Hamilton and subsequently led to what is now referred to the Hamiltonian formulation of classical mechanics.</p>

<p>Hamilton postulated that there exists a function \(H : q \times p \mapsto \mathbb{R}\) – the Hamiltonian. The evolution equations in this framework were given by the following PDE:</p>

\[\begin{align}
    \frac{\partial}{\partial t} q(t) = \frac{\partial H}{\partial p}\\ 
    \frac{\partial}{\partial t} p(t) = -\frac{\partial H}{\partial q}
\end{align}\]

<p>Or more compactly</p>

\[{\partial_t} z := {\partial_t}\begin{bmatrix} q \\ p \end{bmatrix} = \begin{bmatrix} \mathcal{0} &amp;&amp;  \mathcal{I} \\ -\mathcal{I} &amp;&amp; \mathcal{0} \end{bmatrix} \partial_{\begin{bmatrix} q &amp; p \end{bmatrix}^{T}} H(q,p)\]

<p>Given observations \(\left([q(t_1),q(t_2),...,q(t_m)],[p(t_1),p(t_2),...,p(t_m)]\right)\), I am interested in learning \(H\) using a Gaussian process.</p>

<h3 id="method">Method</h3>

<p>Let \(H \sim \mathcal{GP(\mu_z,\kappa(z,z'))}\).</p>]]></content><author><name></name></author><category term="projects" /><category term="SystemsIdentification" /><summary type="html"><![CDATA[Inferring invariants of dynamical systems with gaussian processes.]]></summary></entry><entry><title type="html">Multi-output Gaussian processes</title><link href="https://dynamic-queries.github.io/blog/2023/mogp/" rel="alternate" type="text/html" title="Multi-output Gaussian processes" /><published>2023-07-20T08:00:00+00:00</published><updated>2023-07-20T08:00:00+00:00</updated><id>https://dynamic-queries.github.io/blog/2023/mogp</id><content type="html" xml:base="https://dynamic-queries.github.io/blog/2023/mogp/"><![CDATA[<h3 id="introduction">Introduction</h3>
<p>Gaussian processes are a popular <strong>Bayesian</strong> <strong>learning</strong> framework.</p>

<p>A typical <em>learning problem</em> consists of approximating an approximation \(\hat{f}\) to the function \(f:x \mapsto y \quad \forall x,y \in \Omega \subset \mathbb{R}^d\), given evaluations of the function at certain input points, that is \(\mathcal{D} = \{(x_i, f_i) : i \in \mathbb{N}\}\).</p>

<p><em>Bayesian inference</em> assumes that the input \(x\) and the outputs \(y\) are random variables with some densities \(P_x, P_y\) respectively. These densities are related by</p>

\[P(x,y) = P(y | x) P(x) = P(x | y) P(y)\]

<p>Note that we do not have a closed form for any of these probability functions and estimating probability densities from data is in itself a hard problem. Instead, we make guesses about what these functions might be. Historically, each of these probability functions have their own names, namely</p>

<ol>
  <li>\(P(y)\) – Prior</li>
  <li>\(P(x | y)\) – Likelihood</li>
  <li>\(P(x)\) – Marginal</li>
  <li>\(P(y | x)\) – Posterior</li>
</ol>

<p>Commonly, one assumes a functional form for the prior. The posterior is the updated version of the prior, once we have seen the reference data.</p>

<h3 id="single-output-gp">Single Output GP</h3>

<p>Further, samples \(y,x\) are assumed to be related via</p>

\[y = f(x)\]

\[f:x \mapsto y \quad \forall x,y \in \Omega\]

<p>Here \(f\) is assumed to be any non-linear function. One can project \(x\) onto a higher dimensional space spanned by the bases generated from a kernel function \(\kappa\), also known as the <strong>kernel trick</strong>, where the non-linearity of \(f\) is transformed into a linearity. Without any loss of generality, we can assume that any non-linear function can be transformed into a linear map.</p>

\[y = Wx + b\]

\[W \in \mathbb{R}^{d \times d} \: \textrm{ and } b \in \mathbb{R}^d\]

<p>With this parameteric form for the output, defining a prior for the weights, \(z := \begin{bmatrix} W &amp; b \end{bmatrix}\) is equivalent to defining a prior for the output, \(y\). Therefore in a single output GP, there are three random variables \(x,y,z\). Their joint probability distribution is given as</p>

\[\begin{align}
    P(x,y,z) = P(x,y|z) P(z) = P(z|x,y) P(x,y) \\
 P(z|x,y) = \frac{P(x,y|z) P(z)}{P(x,y)} \\ 
 = \frac{P(y|x,z) P(x,z)}{P(x,y)} \\
  = \frac{P(y|x,z) P(x)P(z)}{P(y|x)P(x)}\\
 P(z|x,y) \propto P(y|x,z) P(z)
 \end{align}\]

<p>Here, \(z \sim \mathcal{N}(0, \Sigma_z)\), that is</p>

\[\begin{align}
    P(z) = \frac{1}{\sqrt{(2\pi)^d |\Sigma_z|}} \exp\left(-\frac{1}{2} z^T \Sigma_z^{-1} z\right)
\end{align}\]

<p>Further, the likelihood is defined with the assumption that each component of \(x\) is independent of one another. This is why this inference procedure is called single output Gaussian process. That is</p>

\[\begin{align}
    P(y|x,z) = \prod_{i=1}^{d} \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{1}{2}\left(\frac{y_i - (W_i x_i + b_i)}{\sigma}\right)^2\right) \\ 
    P(y|x,z) = \frac{1}{(\sqrt{2\pi \sigma^2})^d} \exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^{d} (y_i - (W_i x_i + b_i))^2\right) \\ 
    = \frac{1}{\sqrt{(2\pi)^d |\Sigma_y|}} \exp\left(-\frac{1}{2}(y - (W x + b))^T \Sigma_y^{-1} (y - (W x + b))\right)
\end{align}\]

<p>From this \(P(y|x,z)\) is a multivariate normal distribution \(\mathcal{N}(Wx+b, \Sigma_y)\).</p>

<p>Further one can write down the posterior as</p>

\[\begin{align}
    P(z|x,y) \propto P(y|x,z) P(z) \\ 
    = \frac{1}{(2\pi)^d \sqrt{|\Sigma_z||\Sigma_y|}} \exp\left(-\frac{1}{2} \left[z^T \Sigma_z^{-1} z + (y - (W x + b))^T \Sigma_y^{-1} (y - (W x + b)) \right]\right) \\ 
    = \frac{1}{(2\pi)^d \sqrt{|\Sigma_z||\Sigma_y|}} \exp\left(-\frac{1}{2} \left[z^T \Sigma_z^{-1} z + (y - z x')^T \Sigma_y^{-1} (y - z x') \right]\right)
\end{align}\]]]></content><author><name></name></author><category term="projects" /><category term="SystemsIdentification" /><summary type="html"><![CDATA[Learning mutivariate dynamical systems with GPs]]></summary></entry></feed>