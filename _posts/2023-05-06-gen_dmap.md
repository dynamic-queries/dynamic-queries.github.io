---
layout: post
title: Generative modeling using Diffusion map particle system
date: 2023-05-06 08:00:00-0000
description: Marrying two unlikely algorithms for physical modeling.
tags: GenMod
categories: review
related_posts: false
bibliography: 
urlcolor: "blue"
---
### Introduction

This review aims at providing a self-contained, pedantic, comprehensive overview of deep generative modelling, culminating with discussions on [the recent paper](https://arxiv.org/abs/2304.00200) from Youssef Marzouk.


Generative models are manifestations of this widely held philosophical belief:
<div class="alert alert-block alert-success">
<b>Philosophical Belief 1</b> All natural phenomena are generative. That is to say, there is an underlying generative process for every physical occurance.
</div>


Clearly, there are infinitely many possibilities that may ensue from such a generative process. Assuming that we can enumerate all of these outcomes, say with set $$\Omega$$, then a generative process $$\mathcal{G}$$ underlying a system $$\mathcal{S}$$ can be effectively described using probability theory. Therefore, the problem of generative modelling is in-effect the problem of probability density (distribution) estimation.

In practice, there are several restrictions to effectively estimating probability density, $$p_{\mathcal{G}}(z) \: \forall z \in \Omega$$, from a finite set of observations $$\Omega_f \subset \Omega$$. Regardless, any method $$m$$ that claims to do so must satisfy these conditions.
1. For a suitably chosen functional norm, $$\lim_{\Omega_f \mapsto \Omega} \|p_{\mathcal{G}}^{m}(z) - p_{\mathcal{G}}(z)\|_f \mapsto 0$$
2. It must be computationally effcient to sample from $$p_{\mathcal{G}}^{m}(z)$$.
3. It must be computationally effcient to evaluate $$p_{\mathcal{G}}^{m}(z)$$.

Traditional functional approximation methods can satisfy (1) with an arbitrary level of accuracy, yet fail with respect to (2) and (3) due to the curse of dimensionality. Deep learning models on the other hand, are adept at circumventing this problem. Popular deep-generative models (in no particular order) include

<div class="alert alert-block alert-success">
   1. Adversarial Generative Models <br>
   2. Variational Autoencoders <br>
   3. Normalizing Flows <br>
   4. Score based Diffusion Models <br>
   5. Autoregressive Models <br>
   6. Energy based Models <br>
</div>
to name a few. In the ensuing discussion, we discuss the algorithmic procedures, their implications (where applicable), applications to MNIST datasets and the relative merits and drawbacks of these models.